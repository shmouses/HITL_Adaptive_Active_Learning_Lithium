{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human-AI Synergy in Adaptive Active Learning for Continuous Lithium Carbonate Crystallization Optimization\n",
    "\n",
    "**Shayan Mousavi Masouleh¹²*, Corey A. Sanz³, Ryan P. Jansonius³, Cara Cronin³, Jason E. Hein³, Jason Hattrick-Simpers¹***\n",
    "\n",
    "¹ CanmetMATERIALS, Natural Resources Canada, Hamilton, ON, Canada  \n",
    "² Clean Energy Innovation, National Research Council of Canada, Mississauga, ON, Canada  \n",
    "³ Telescope Innovations, Vancouver, BC, Canada\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "This Jupyter Notebook serves as supplementary material for the research paper, *\"Human-AI Synergy in Adaptive Active Learning for Continuous Lithium Carbonate Crystallization Optimization.\"* It provides the executable code to reproduce the analyses, visualizations, and key findings presented in the publication.\n",
    "\n",
    "The primary goal of our research is to demonstrate a Human-in-the-Loop Active Learning (HITL-AL) framework that optimizes the continuous crystallization of lithium carbonate ($Li_2CO_3$) from low-grade brines. This is particularly relevant for resources like the Smackover Formation, which are rich in lithium but also contain high levels of impurities such as magnesium (Mg), calcium (Ca), and potassium (K).\n",
    "\n",
    "This notebook is structured to follow the narrative of the paper, guiding the user through the following key sections:\n",
    "\n",
    "1. **Setup and Configuration**: Loading necessary libraries, defining utility functions, and configuring the computational environment for reproducible research.\n",
    "\n",
    "2. **Auxiliary Functions**: Core data processing, surrogate space generation, statistical analysis, and optimization functions that implement the HITL-AL workflow described in the **Methods** section of the paper.\n",
    "\n",
    "3. **Data Loading and Preparation**: Loading and cleaning the experimental dataset from multiple SIFT experimental campaigns, including progressive data loading to simulate the active learning timeline.\n",
    "\n",
    "4. **Data and Results Inspection**: Comprehensive statistical analysis of the experimental data to understand relationships between input parameters and outcomes, including feature importance analysis and correlation studies.\n",
    "\n",
    "5. **Exploration Phase (Pareto Frontier Analysis)**: Demonstrating how Gaussian Process Regression (GPR) and the NSGA-II algorithm were used to explore the parameter space and identify conditions that minimize Mg and Ca impurities simultaneously.\n",
    "\n",
    "6. **The Human-in-the-Loop Insight (Random Walk Analysis)**: Showcasing the pivotal discovery where a model-guided random walk revealed the counterintuitive but critical role of the cold reactor temperature ($T_{cold}$) in magnesium removal.\n",
    "\n",
    "7. **Exploitation Phase (Decision Boundary Refinement)**: Detailing how Gaussian Process Classification (GPC) was used to precisely map the decision boundary between battery-grade and non-battery-grade outcomes, including ray tracing for optimal experiment selection.\n",
    "\n",
    "8. **Final Results and Analysis**: Presenting the final model performance, key parameter distributions, and comprehensive comparative analysis that underscores the efficiency of the HITL-AL approach.\n",
    "\n",
    "9. **Comparative Simulation**: Quantifying the benefits of the HITL-AL approach through computational experiments comparing its efficacy against Bayesian optimization and random sampling in both informed and uninformed scenarios.\n",
    "\n",
    "By providing this interactive and reproducible material, we aim to offer a transparent and in-depth view of our methodology and results, facilitating further research and application in the field of materials science and chemical process optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup: Libraries and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Importing Libraries\n",
    "\n",
    "This cell imports all the necessary Python libraries for data manipulation, scientific computing, machine learning, and visualization. We configure the notebook to display plots inline and set a consistent random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook configuration\n",
    "%matplotlib inline\n",
    "\n",
    "# Standard library imports\n",
    "import datetime\n",
    "import math\n",
    "import platform\n",
    "import random\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "# Third-party scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.stats import qmc, zscore\n",
    "from math import ceil\n",
    "\n",
    "# Visualization\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Machine learning and optimization\n",
    "import optuna\n",
    "import shap\n",
    "import sklearn\n",
    "from sklearn import (\n",
    "    ensemble, gaussian_process, linear_model, neighbors, preprocessing, svm, tree\n",
    ")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor, GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import (\n",
    "    ConstantKernel, DotProduct, Matern, RationalQuadratic, RBF, WhiteKernel\n",
    ")\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.metrics import (\n",
    "    f1_score, make_scorer, mean_absolute_error, mean_absolute_percentage_error,\n",
    "    mean_squared_error, pairwise, r2_score\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV, ShuffleSplit, cross_val_score, train_test_split\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# Multi-objective optimization\n",
    "import platypus\n",
    "from platypus import NSGAII, Problem, Real, nondominated\n",
    "\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "import tqdm\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set matplotlib style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "from io import StringIO, BytesIO\n",
    "from typing import Any, Iterable, Optional, Union\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Library Versions\n",
    "\n",
    "To ensure full reproducibility, we document the versions of the key libraries used in this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing versions of the loaded modules\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Scipy version: {scipy.__version__}\")\n",
    "print(f\"Matplotlib version: {matplotlib.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"Tqdm version: {tqdm.__version__}\")\n",
    "print(f\"Platypus-Opt version: {platypus.__version__}\")\n",
    "print(f\"Optuna version: {optuna.__version__}\")\n",
    "print(f\"SHAP version: {shap.__version__}\")\n",
    "print(f\"Notebook executed on: {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Auxiliary Functions\n",
    "\n",
    "This section contains the definitions of helper functions used throughout the notebook. These functions are categorized based on their role in the HITL-AL workflow described in the **Methods** section of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Extraction and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sift_data_extractor(data, ppm=True, pH=False, slurry=False, score=False, NaOH=False):\n",
    "    \"\"\"Extract and standardize experimental data from raw SIFT files.\"\"\"\n",
    "    new_data = pd.DataFrame()\n",
    "    if 'Experiment ID' in data.columns:\n",
    "        new_data['experiment_id'] = data['Experiment ID']\n",
    "    if 'Successful Experiment' in data.columns:\n",
    "        new_data['success'] = data['Successful Experiment']\n",
    "    else:\n",
    "        new_data['success'] = 'T'\n",
    "    if score:\n",
    "        if 'Success score' in data.columns:\n",
    "            new_data['score'] = data['Success score']\n",
    "        else:\n",
    "            new_data['score'] = 1\n",
    "    if 'T cold (deg C)' in data.columns:\n",
    "        new_data['T_cold'] = data['T cold (deg C)'].astype(float)\n",
    "    if 'T hot (deg C)' in data.columns:\n",
    "        new_data['T_hot'] = data['T hot (deg C)'].astype(float)\n",
    "    if 'flow rate (mL/min)' in data.columns:\n",
    "        new_data['flow_rate'] = data['flow rate (mL/min)'].astype(float)\n",
    "    if 'slurry concentration (g total solid/100 mL)' in data.columns:\n",
    "        new_data['slurry_concentration'] = data['slurry concentration (g total solid/100 mL)'].astype(float)\n",
    "    if pH and 'pH of slurry (initial)' in data.columns:\n",
    "        new_data['pH'] = data['pH of slurry (initial)'].astype(float)\n",
    "    if NaOH and 'millimoles NaOH added' in data.columns:\n",
    "        new_data['NaOH'] = data['millimoles NaOH added'].astype(float)\n",
    "    if slurry:\n",
    "        slurry_components = {\n",
    "            'slurry concentration (g CaCO3/100 mL)': 'scl_Ca',\n",
    "            'slurry concentration (g K2CO3/100 mL)': 'scl_K',\n",
    "            'slurry concentration (g Li2CO3/100 mL)': 'scl_Li',\n",
    "            'slurry concentration (g (Mg(CO3))4 Mg(OH)2/100 mL)': 'scl_Mg',\n",
    "            'slurry concentration (g Na2CO3/100 mL)': 'scl_Na',\n",
    "            'slurry concentration (g SrCO3/100 mL)': 'scl_Sr'\n",
    "        }\n",
    "        for column, new_column_name in slurry_components.items():\n",
    "            if column in data.columns:\n",
    "                new_data[new_column_name] = data[column].astype(float)\n",
    "    if ppm:\n",
    "        ppm_components = {\n",
    "            'B (ppm)': 'init_B', 'Ca (ppm)': 'init_Ca', 'K (ppm)': 'init_K',\n",
    "            'Li (ppm)': 'init_Li', 'Mg (ppm)': 'init_Mg', 'Na (ppm)': 'init_Na',\n",
    "            'Si (ppm)': 'init_Si', 'Sr (ppm)': 'init_Sr', 'Li2CO3 purity (%)': 'init_Li_purity',\n",
    "            'B (ppm).1': 'fini_B', 'Ca (ppm).1': 'fini_Ca', 'K (ppm).1': 'fini_K',\n",
    "            'Li (ppm).1': 'fini_Li', 'Mg (ppm).1': 'fini_Mg', 'Na (ppm).1': 'fini_Na',\n",
    "            'Si (ppm).1': 'fini_Si', 'Sr (ppm).1': 'fini_Sr', 'Li2CO3 purity (%).1': 'fini_Li_purity'\n",
    "        }\n",
    "        for column, new_column_name in ppm_components.items():\n",
    "            if column in data.columns:\n",
    "                new_data[new_column_name] = data[column].astype(float)\n",
    "    else:\n",
    "        percentage_components = {\n",
    "            'B (%)': 'init_B', 'Ca (%)': 'init_Ca', 'K (%)': 'init_K', 'Li (%)': 'init_Li',\n",
    "            'Mg (%)': 'init_Mg', 'Na (%)': 'init_Na', 'Si (%)': 'init_Si', 'Sr (%)': 'init_Sr',\n",
    "            'B (%).1': 'fini_B', 'Ca (%).1': 'fini_Ca', 'K (%).1': 'fini_K', 'Li (%).1': 'fini_Li',\n",
    "            'Mg (%).1': 'fini_Mg', 'Na (%).1': 'fini_Na', 'Si (%).1': 'fini_Si', 'Sr (%).1': 'fini_Sr'\n",
    "        }\n",
    "        for column, new_column_name in percentage_components.items():\n",
    "            if column in data.columns:\n",
    "                new_data[new_column_name] = data[column].astype(float)\n",
    "    return new_data\n",
    "\n",
    "def successful_sift_extraction(data):\n",
    "    \"\"\"Filter SIFT experimental data to include only successful experiments.\"\"\"\n",
    "    new_data = data[data['success'].isin(['T'])].copy()\n",
    "    return new_data\n",
    "\n",
    "def ppm_threshold(element):\n",
    "    \"\"\"This function returns the battery-grade threshold for each element in ppm.\"\"\"\n",
    "    fini_threshold ={\n",
    "        'fini_Ca': 160,\n",
    "        'fini_K': 10,\n",
    "        'fini_Mg': 80,\n",
    "        'fini_Na': 500,\n",
    "        'fini_Si': 40\n",
    "    }\n",
    "    return fini_threshold.get(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Surrogate Space Generation\n",
    "\n",
    "As described in **Section 2.2.3 (Informed Design and Adjustments)** of the paper, we construct a surrogate space to represent the multidimensional experimental conditions. We use Latin Hypercube Sampling (LHS) for its efficient and uniform coverage of the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latin_hypercube_sample(n_points, dimension, lower_bounds, upper_bounds, seed=None):\n",
    "    \"\"\"Generate Latin Hypercube samples for experimental design.\"\"\"\n",
    "    sampler = qmc.LatinHypercube(d=dimension, seed=seed, optimization='random-cd')\n",
    "    sample = sampler.random(n=n_points)\n",
    "    scaled_sample = qmc.scale(sample, lower_bounds, upper_bounds)\n",
    "    return scaled_sample\n",
    "\n",
    "def uniform_random_sample(n_points, dimension, lower_bounds, upper_bounds, seed=None):\n",
    "    \"\"\"Generate uniform random samples for experimental design.\"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    samples = np.random.uniform(low=lower_bounds, high=upper_bounds, size=(n_points, dimension))\n",
    "    return samples\n",
    "\n",
    "def sift_lhs_sample(n_points, bounds=None, lhs_sampler=True, seed=None, delta_T=0):\n",
    "    \"\"\"\n",
    "    Generate a validated experimental design for SIFT experiments using LHS or uniform sampling.\n",
    "    This function enforces physical constraints such as T_hot > T_cold and normalizes\n",
    "    initial concentrations to realistic values.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "    if bounds is None:\n",
    "        bounds = {\n",
    "            \"T_cold\": (10, 90), \"T_hot\": (20, 95), \"flow_rate\": (0.5, 6),\n",
    "            \"slurry_concentration\": (1.5, 10), \"init_Ca\": (1000, 300000),\n",
    "            \"init_K\": (300, 3000), \"init_Li\": (15000, 300000),\n",
    "            \"init_Mg\": (10, 20000), \"init_Na\": (300, 20000)\n",
    "        }\n",
    "\n",
    "    l_bounds = [v[0] for v in bounds.values()]\n",
    "    u_bounds = [v[1] for v in bounds.values()]\n",
    "    columns = list(bounds.keys())\n",
    "    initial_n_points = int(4 * n_points + 50) if lhs_sampler else int(4 * n_points + 10)\n",
    "\n",
    "    if lhs_sampler:\n",
    "        sample = latin_hypercube_sample(initial_n_points, len(bounds), l_bounds, u_bounds, seed=seed)\n",
    "    else:\n",
    "        sample = uniform_random_sample(initial_n_points, len(bounds), l_bounds, u_bounds, seed=seed)\n",
    "\n",
    "    exp_grid = pd.DataFrame(sample, columns=columns)\n",
    "    mask = exp_grid['T_cold'] > exp_grid['T_hot']\n",
    "    exp_grid.loc[mask, ['T_cold', 'T_hot']] = exp_grid.loc[mask, ['T_hot', 'T_cold']].values\n",
    "\n",
    "    scaling_factors = {'init_Ca': 0.404, 'init_K': 0.5655, 'init_Li': 0.1878, 'init_Mg': 0.5094, 'init_Na': 0.4338}\n",
    "    init_sum = sum(exp_grid[col] / scaling_factors.get(col, 1) for col in scaling_factors)\n",
    "    for col in scaling_factors:\n",
    "        if col in exp_grid.columns:\n",
    "             exp_grid[col] = 1_000_000 * exp_grid[col] / init_sum\n",
    "\n",
    "    for parameter, (lower_bound, upper_bound) in bounds.items():\n",
    "        exp_grid = exp_grid[exp_grid[parameter].between(lower_bound, upper_bound)]\n",
    "\n",
    "    exp_grid['T_hot'] += delta_T\n",
    "    exp_grid_final = exp_grid.sample(frac=1, random_state=seed).reset_index(drop=True).head(n_points)\n",
    "    return exp_grid_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Statistical and ML Model Analysis\n",
    "\n",
    "These functions implement the analyses described in **Section 2.2.1 (Data and Results Inspection)** of the paper. They include tools for hyperparameter tuning, model training, SHAP analysis, and other statistical evaluations used to provide insights for the \"Human Interpretation\" phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(X, y, n_trials=25):\n",
    "    \"\"\"Optimize RandomForestRegressor hyperparameters using Optuna.\"\"\"\n",
    "    def objective(trial):\n",
    "        param = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "            'random_state': 42\n",
    "        }\n",
    "        model = RandomForestRegressor(**param)\n",
    "        score = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error').mean()\n",
    "        return -score\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "    return study.best_params\n",
    "\n",
    "def train_model(X, y, best_params):\n",
    "    \"\"\"Train a RandomForestRegressor model with specified parameters.\"\"\"\n",
    "    model = RandomForestRegressor(**best_params, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def shap_feature_importance(model, X):\n",
    "    \"\"\"Create a SHAP feature importance plot.\"\"\"\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    shap.summary_plot(shap_values, X, plot_type=\"bar\", show=False)\n",
    "    plt.xlabel(\"mean |SHAP value| (average impact on model output magnitude)\")\n",
    "    plt.title(\"SHAP Feature Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_data_analysis(data, pair_plot=False):\n",
    "    \"\"\"Create pair plots and a correlation matrix heatmap for the given data.\"\"\"\n",
    "    if pair_plot:\n",
    "        sns.pairplot(data)\n",
    "        plt.show()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = data.corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title(\"Pearson Correlation Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_partial_dependence(model, X):\n",
    "    \"\"\"Generate and display partial dependence plots for each feature.\"\"\"\n",
    "    features = X.columns\n",
    "    n_cols = 3\n",
    "    n_rows = ceil(len(features) / n_cols)\n",
    "    fig, axs = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(n_cols * 5, n_rows * 4))\n",
    "    fig.suptitle('Partial Dependence Plots', fontsize=16)\n",
    "    for i, feature in enumerate(features):\n",
    "        ax = axs.flat[i]\n",
    "        PartialDependenceDisplay.from_estimator(model, X, features=[feature], ax=ax, grid_resolution=20)\n",
    "        ax.set_title(feature)\n",
    "    for ax in axs.flat[len(features):]:\n",
    "        ax.set_visible(False)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "def plot_mutual_information(X, y):\n",
    "    \"\"\"Calculate and display a mutual information plot for features.\"\"\"\n",
    "    mutual_info = mutual_info_regression(X, y, random_state=42)\n",
    "    mi_series = pd.Series(mutual_info, index=X.columns).sort_values()\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    mi_series.plot(kind='barh', color='blue')\n",
    "    plt.xlabel('Mutual Information Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Feature Importance (Mutual Information)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def sensitivity_analysis(model, X, y, feature_names):\n",
    "    \"\"\"Perform and plot a normalized sensitivity analysis.\"\"\"\n",
    "    importances = {}\n",
    "    baseline_mse = mean_squared_error(y, model.predict(X))\n",
    "    for feature in feature_names:\n",
    "        X_perturbed = X.copy()\n",
    "        X_perturbed[feature] += X_perturbed[feature].std()\n",
    "        mse_perturbed = mean_squared_error(y, model.predict(X_perturbed))\n",
    "        change_in_mse = abs(mse_perturbed - baseline_mse)\n",
    "        normalized_change = change_in_mse / baseline_mse if baseline_mse != 0 else 0\n",
    "        importances[feature] = normalized_change\n",
    "    sorted_importances = pd.Series(importances).sort_values()\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sorted_importances.plot(kind='barh', color='purple')\n",
    "    plt.xlabel('Normalized Change in MSE')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Normalized Sensitivity Analysis')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return importances\n",
    "\n",
    "def perform_full_analysis(data, feature_columns, target_columns, pair_plot=False):\n",
    "    \"\"\"Conduct a full statistical analysis pipeline for a given target.\"\"\"\n",
    "    for target in target_columns:\n",
    "        print(f\"\\n--- Analyzing for Target: {target} ---\")\n",
    "        y = data[target]\n",
    "        X = data[feature_columns]\n",
    "        print(\"Tuning hyperparameters...\")\n",
    "        best_params = tune_hyperparameters(X, y)\n",
    "        print(f\"Best parameters found: {best_params}\")\n",
    "        print(\"Training model...\")\n",
    "        model = train_model(X, y, best_params)\n",
    "        print(\"Generating Data Analysis Plots:\")\n",
    "        plot_data_analysis(data[feature_columns + [target]], pair_plot=pair_plot)\n",
    "        print(\"Generating Partial Dependence Plots:\")\n",
    "        plot_partial_dependence(model, X)\n",
    "        print(\"Generating SHAP Feature Importance Plot:\")\n",
    "        shap_feature_importance(model, X)\n",
    "        print(\"Generating Mutual Information Plot:\")\n",
    "        plot_mutual_information(X, y)\n",
    "        print(\"Performing Sensitivity Analysis:\")\n",
    "        sensitivity_analysis(model, X, y, feature_columns)\n",
    "\n",
    "def find_missing_data(data):\n",
    "    \"\"\"Print a summary of missing data per column.\"\"\"\n",
    "    missing_data = data.isna().sum()\n",
    "    if missing_data.sum() > 0:\n",
    "        print(\"Missing data counts per column:\\n\", missing_data[missing_data > 0])\n",
    "        for column in data.columns[data.isna().any()].tolist():\n",
    "            print(f\"\\nNaNs in column '{column}':\")\n",
    "            print(data[data[column].isna()])\n",
    "    else:\n",
    "        print(\"No missing data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Pareto Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platypus import NSGAII, Problem, Real, nondominated\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "\n",
    "def optimize_pareto_front(\n",
    "    grid_gpr: pd.DataFrame,\n",
    "    obj1_col: str = \"fini_Mg\",\n",
    "    obj2_col: str = \"fini_Ca\",\n",
    "    min_unique_points: int = 30,\n",
    "    population_size: int = 500,\n",
    "    generations: int = 10000\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Perform NSGA-II optimization to extract a Pareto front from `grid_gpr`\n",
    "    based on the specified objective columns.\n",
    "\n",
    "    Args:\n",
    "        grid_gpr (pd.DataFrame): DataFrame containing the surrogate predictions.\n",
    "        obj1_col (str): Column name for the first objective.\n",
    "        obj2_col (str): Column name for the second objective.\n",
    "        min_unique_points (int): Minimum number of unique Pareto points to collect.\n",
    "        population_size (int): Size of the NSGA-II population.\n",
    "        generations (int): Number of generations to run per optimization cycle.\n",
    "\n",
    "    Returns:\n",
    "        pareto_front_unique (pd.DataFrame): Unique Pareto-optimized rows with only objective columns.\n",
    "        grid_gpr_unique (pd.DataFrame): Full unique rows of `grid_gpr` for the Pareto-optimized indices.\n",
    "    \"\"\"\n",
    "    \n",
    "    def objective(x):\n",
    "        idx = [int(i) for i in x]\n",
    "        f1 = grid_gpr.loc[idx, obj1_col].values\n",
    "        f2 = grid_gpr.loc[idx, obj2_col].values\n",
    "        return [f1[0], f2[0]]\n",
    "\n",
    "    # Define the problem\n",
    "    problem = Problem(1, 2)\n",
    "    problem.types[:] = Real(0, grid_gpr.shape[0] - 1)\n",
    "    problem.function = objective\n",
    "\n",
    "    # Prepare the dataframe with renamed objective columns\n",
    "    df = grid_gpr[[obj1_col, obj2_col]].rename(columns={obj1_col: 'obj1', obj2_col: 'obj2'})\n",
    "\n",
    "    pareto_front_unique = pd.DataFrame()\n",
    "    pareto_indices_unique = set()\n",
    "    algorithm = NSGAII(problem, population_size=population_size)\n",
    "\n",
    "    while len(pareto_front_unique) < min_unique_points:\n",
    "        algorithm.run(generations)\n",
    "\n",
    "        front = nondominated(algorithm.result)\n",
    "        pareto_indices = [int(solution.variables[0]) for solution in front]\n",
    "\n",
    "        pareto_indices_unique.update(pareto_indices)\n",
    "        pareto_front = df.iloc[pareto_indices]\n",
    "        pareto_front_unique = pd.concat([pareto_front_unique, pareto_front], ignore_index=False)        \n",
    "        pareto_front_unique = pareto_front_unique.drop_duplicates()\n",
    "\n",
    "        if len(pareto_front_unique) < min_unique_points:\n",
    "            print(f\"Warning: Only {len(pareto_front_unique)} unique points found. Re-running optimization...\")\n",
    "            algorithm = NSGAII(problem, population_size=population_size)\n",
    "\n",
    "    grid_gpr_unique = grid_gpr.iloc[list(pareto_indices_unique)].drop_duplicates()\n",
    "\n",
    "    print(\"Unique Pareto front:\")\n",
    "    print(pareto_front_unique.head(min_unique_points))\n",
    "    print(\"\\nCorresponding rows from grid_gpr:\")\n",
    "    print(grid_gpr_unique.head(min_unique_points))\n",
    "\n",
    "    return pareto_front_unique, grid_gpr_unique\n",
    "\n",
    "\n",
    "\n",
    "def pareto_gpr_prediction(file_path_exp: str,\n",
    "                          file_path_lhs: str,\n",
    "                          multi_dimensional: bool = True,\n",
    "                          delta_T: bool = True,\n",
    "                          alpha: float = 1e-10):\n",
    "    \"\"\"\n",
    "    Perform Gaussian Process Regression (GPR) to predict post-crystallization impurity concentrations\n",
    "    using experimental data and a test design space (e.g., Latin Hypercube Sampling).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path_exp : str\n",
    "        Path to the CSV file containing observed experimental data (features + target impurities).\n",
    "\n",
    "    file_path_lhs : str\n",
    "        Path to the CSV file containing unseen design points for prediction (e.g., LHS sample).\n",
    "\n",
    "    multi_dimensional : bool, default=True\n",
    "        If True, uses a multi-output GPR model to predict all targets simultaneously.\n",
    "        If False, fits one independent GPR model per target impurity.\n",
    "\n",
    "    delta_T : bool, default=True\n",
    "        If True, computes and includes the temperature difference (T_hot - T_cold) as a feature.\n",
    "\n",
    "    alpha : float, default=1e-10\n",
    "        Value of alpha used for regularization in GPR to prevent overfitting in low-noise settings.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    grid_gpr : pd.DataFrame\n",
    "        GPR-predicted impurity concentrations for each row in the test design space (X_test).\n",
    "    \n",
    "    X_train : pd.DataFrame\n",
    "        Final processed training feature matrix used to fit GPR.\n",
    "    \n",
    "    y_train : pd.DataFrame\n",
    "        Training targets (impurity levels after crystallization).\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - This function assumes that the experimental file includes both input features and observed outputs.\n",
    "    - It supports both multi-output and independent GPR strategies.\n",
    "    - Features are standardized using StandardScaler to ensure stable model behavior.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load experimental data\n",
    "    data = pd.read_csv(file_path_exp)\n",
    "    print(\"Initial experimental data sample:\")\n",
    "    print(data.head(5))\n",
    "\n",
    "    # Drop irrelevant or noisy columns, if present\n",
    "    columns_to_drop = [\n",
    "        'init_Li_purity', 'fini_Li_purity', 'init_B', 'init_Si', 'fini_B', 'fini_Si',\n",
    "        'init_Sr', 'fini_Sr', 'scl_Ca', 'scl_K', 'scl_Li', 'scl_Mg', 'scl_Na', 'scl_Sr'\n",
    "    ]\n",
    "    for column in columns_to_drop:\n",
    "        if column in data.columns:\n",
    "            data = data.drop(column, axis=1)\n",
    "\n",
    "    # Compute delta_T (optional)\n",
    "    cleaned_data = data.copy()\n",
    "    if delta_T:\n",
    "        cleaned_data['delta_T'] = cleaned_data['T_hot'] - cleaned_data['T_cold']\n",
    "        \n",
    "\n",
    "    # Drop rows with missing values\n",
    "    cleaned_data = cleaned_data.dropna()\n",
    "\n",
    "    # Define input features (X) for training\n",
    "    base_features = ['T_cold', 'T_hot', 'flow_rate', 'slurry_concentration',\n",
    "                     'init_Ca', 'init_K', 'init_Li', 'init_Mg', 'init_Na']\n",
    "    if delta_T:\n",
    "        feature_cols = base_features[:2] + ['delta_T'] + base_features[2:]\n",
    "    else:\n",
    "        feature_cols = base_features\n",
    "\n",
    "    X_train = cleaned_data[feature_cols].copy()\n",
    "    y_train = cleaned_data[['fini_Ca', 'fini_K', 'fini_Li', 'fini_Mg', 'fini_Na']].copy()\n",
    "\n",
    "    # Load test data and prepare features\n",
    "    X_test = pd.read_csv(file_path_lhs)\n",
    "    if delta_T:\n",
    "        X_test['delta_T'] = X_test['T_hot'] - X_test['T_cold']\n",
    "    X_test = X_test[feature_cols].copy()\n",
    "\n",
    "    print(\"Processed training features sample:\")\n",
    "    print(X_train.head(5))\n",
    "\n",
    "    # Standardize features for both train and test\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Perform GPR\n",
    "    if multi_dimensional:\n",
    "        # Use multi-output GPR (single model)\n",
    "        gpr_model = GaussianProcessRegressor(kernel=Matern(length_scale=1.0, nu=1.5),\n",
    "                                             alpha=alpha,\n",
    "                                             n_restarts_optimizer=1)\n",
    "        gpr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        y_pred, y_std = gpr_model.predict(X_test_scaled, return_std=True)\n",
    "        gpr_pred_df = pd.DataFrame(y_pred, columns=y_train.columns)\n",
    "\n",
    "        # Combine predictions with original input space\n",
    "        grid_gpr = pd.concat([X_test.reset_index(drop=True), gpr_pred_df], axis=1)\n",
    "\n",
    "    else:\n",
    "        # Use one GPR model per output target\n",
    "        gpr_model = {}\n",
    "        grid_gpr = {}\n",
    "\n",
    "        for target in y_train.columns:\n",
    "            gpr_model[target] = GaussianProcessRegressor(kernel=Matern(length_scale=1.0, nu=1.5),\n",
    "                                                         alpha=alpha,\n",
    "                                                         n_restarts_optimizer=10)\n",
    "            gpr_model[target].fit(X_train_scaled, y_train[target])\n",
    "            y_pred = gpr_model[target].predict(X_test_scaled)\n",
    "            grid_gpr[target] = y_pred\n",
    "\n",
    "        grid_gpr = pd.DataFrame(grid_gpr)\n",
    "\n",
    "    return grid_gpr, X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Random Walk at Pareto Frontier Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wobble_dataframe(df, n, wobble_factor=0.25):\n",
    "    \"\"\"\n",
    "    Generate a new dataframe by applying a random 'wobble' to each value of the first row of the input dataframe.\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): The input dataframe from which the first row is used as a base for generation.\n",
    "    n (int): The number of 'wobbled' rows to generate.\n",
    "    wobble_factor (float, optional): The maximum proportion of the original value by which to randomly vary. \n",
    "        Defaults to 0.25.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A new dataframe containing 'n' rows where each value is a 'wobbled' version of the first row's values,\n",
    "        with adjusted initial concentrations to maintain a defined sum constraint and corrected temperature values.\n",
    "    \"\"\"\n",
    "    # Fixing the random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    new_rows = []\n",
    "    for _ in range(n):\n",
    "        wobbled_row = {}\n",
    "        first_row = df.iloc[0]  # Use iloc to get the first row, regardless of its index\n",
    "        for column in df.columns:\n",
    "            value = first_row[column]\n",
    "            # Apply the wobble factor\n",
    "            wobbled_value = value + (np.random.uniform(-wobble_factor, wobble_factor) * value)\n",
    "            wobbled_row[column] = wobbled_value\n",
    "        new_rows.append(wobbled_row)\n",
    "    \n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    \n",
    "    # Compute the sum of the scaled initial concentrations\n",
    "    init_sum = new_df['init_Ca']/0.404 + new_df['init_K']/0.5655 + new_df['init_Li']/0.1878 + new_df['init_Mg']/0.5094 + new_df['init_Na']/0.4338\n",
    "\n",
    "    new_df_norm = new_df.copy()\n",
    "    # Rescale the initial concentrations to enforce the sum constraint\n",
    "    new_df_norm['init_Ca'] = 1000000 * new_df['init_Ca'] / init_sum\n",
    "    new_df_norm['init_K'] = 1000000 * new_df['init_K'] / init_sum\n",
    "    new_df_norm['init_Li'] = 1000000 * new_df['init_Li'] / init_sum\n",
    "    new_df_norm['init_Mg'] = 1000000 * new_df['init_Mg'] / init_sum\n",
    "    new_df_norm['init_Na'] = 1000000 * new_df['init_Na'] / init_sum\n",
    "    \n",
    "    # Correct temperatures if they are in the wrong order\n",
    "    swap_values = new_df_norm.loc[new_df_norm['T_cold'] > new_df_norm['T_hot'], ['T_hot', 'T_cold']].values\n",
    "    new_df_norm.loc[new_df_norm['T_cold'] > new_df_norm['T_hot'], ['T_cold', 'T_hot']] = swap_values\n",
    "    \n",
    "    # Calculate the temperature difference\n",
    "    new_df_norm['delta_T'] = new_df_norm['T_hot'] - new_df_norm['T_cold']\n",
    "    \n",
    "    return new_df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sift_data_extractor(data, ppm=True, pH=False, slurry=False, score=False, NaOH=False):\n",
    "#     \"\"\"\n",
    "#     Extracts and standardizes experimental data from a raw data file.\n",
    "\n",
    "#     This function processes raw data from lithium carbonate crystallization\n",
    "#     experiments, converting it into a standardized format with consistent column names\n",
    "#     and data types for easier analysis.\n",
    "\n",
    "#     Parameters:\n",
    "#     - data (pd.DataFrame): Raw experimental data.\n",
    "#     - ppm (bool): If True, extracts concentration data in ppm. Otherwise, in percent.\n",
    "#     - pH (bool): If True, includes pH data.\n",
    "#     - slurry (bool): If True, includes individual slurry component concentrations.\n",
    "#     - score (bool): If True, includes expert-assigned quality scores.\n",
    "#     - NaOH (bool): If True, includes NaOH addition data.\n",
    "\n",
    "#     Returns:\n",
    "#     - pd.DataFrame: A standardized dataset with consistent column names.\n",
    "#     \"\"\"\n",
    "#     new_data = pd.DataFrame()\n",
    "    \n",
    "#     # Standardize column names (lowercase with underscores)\n",
    "#     column_map = {col: col.strip().replace(' ', '_').replace('(', '').replace(')', '').replace('/', '_').lower() for col in data.columns}\n",
    "#     data.rename(columns=column_map, inplace=True)\n",
    "\n",
    "#     if 'experiment_id' in data.columns:\n",
    "#         new_data['experiment_id'] = data['experiment_id']\n",
    "#     if 'successful_experiment' in data.columns:\n",
    "#         new_data['success'] = data['successful_experiment']\n",
    "#     else:\n",
    "#         new_data['success'] = 'T'\n",
    "\n",
    "#     if score:\n",
    "#         if 'success_score' in data.columns:\n",
    "#             new_data['score'] = data['success_score']\n",
    "#         else:\n",
    "#             new_data['score'] = 1\n",
    "            \n",
    "#     # Mapping for process parameters\n",
    "#     param_map = {\n",
    "#         't_cold_deg_c': 'T_cold',\n",
    "#         't_hot_deg_c': 'T_hot',\n",
    "#         'flow_rate_ml_min': 'flow_rate',\n",
    "#         'slurry_concentration_g_total_solid_100_ml': 'slurry_concentration'\n",
    "#     }\n",
    "#     for old, new in param_map.items():\n",
    "#         if old in data.columns:\n",
    "#             new_data[new] = pd.to_numeric(data[old], errors='coerce')\n",
    "\n",
    "#     # Mapping for concentration data (ppm or %)\n",
    "#     unit = 'ppm' if ppm else '%'\n",
    "#     elements = ['B', 'Ca', 'K', 'Li', 'Mg', 'Na', 'Si', 'Sr']\n",
    "#     for el in elements:\n",
    "#         # Initial concentrations\n",
    "#         col_init = f'{el.lower()}_{unit}'\n",
    "#         if col_init in data.columns:\n",
    "#             new_data[f'init_{el}'] = pd.to_numeric(data[col_init], errors='coerce')\n",
    "#         # Final concentrations\n",
    "#         col_final = f'{el.lower()}_{unit}.1'\n",
    "#         if col_final in data.columns:\n",
    "#             new_data[f'fini_{el}'] = pd.to_numeric(data[col_final], errors='coerce')\n",
    "    \n",
    "#     # Purity\n",
    "#     if f'li2co3_purity_%' in data.columns:\n",
    "#         new_data['init_Li_purity'] = pd.to_numeric(data['li2co3_purity_%'], errors='coerce')\n",
    "#     if f'li2co3_purity_%.1' in data.columns:\n",
    "#         new_data['fini_Li_purity'] = pd.to_numeric(data['li2co3_purity_%.1'], errors='coerce')\n",
    "\n",
    "#     return new_data\n",
    "\n",
    "# def successful_sift_extraction(data):\n",
    "#     \"\"\"\n",
    "#     Filters SIFT experimental data to include only successful experiments.\n",
    "#     \"\"\"\n",
    "#     return data[data['success'].isin(['T', 't', True])].copy()\n",
    "\n",
    "# def ppm_threshold(element):\n",
    "#     \"\"\"Returns the battery-grade threshold for a given element in ppm.\"\"\"\n",
    "#     fini_threshold ={\n",
    "#         'fini_Ca': 160,\n",
    "#         'fini_K': 10,\n",
    "#         'fini_Mg': 80,\n",
    "#         'fini_Na': 500,\n",
    "#         'fini_Si': 40\n",
    "#     }\n",
    "#     return fini_threshold.get(element, None)\n",
    "\n",
    "# def latin_hypercube_sample(n_points, dimension, lower_bounds, upper_bounds, seed=None):\n",
    "#     \"\"\"Generate Latin Hypercube samples for experimental design.\"\"\"\n",
    "#     sampler = qmc.LatinHypercube(d=dimension, seed=seed, optimization='random-cd')\n",
    "#     sample = sampler.random(n=n_points)\n",
    "#     return qmc.scale(sample, lower_bounds, upper_bounds)\n",
    "\n",
    "# def sift_lhs_sample(n_points, bounds=None, lhs_sampler=True, seed=None, delta_T=0):\n",
    "#     \"\"\"Generates an experimental design for SIFT using Latin Hypercube Sampling.\"\"\"\n",
    "#     if seed is not None:\n",
    "#         np.random.seed(seed)\n",
    "#         random.seed(seed)\n",
    "#     if bounds is None:\n",
    "#         bounds = {\n",
    "#             \"T_cold\": (10, 90), \"T_hot\": (20, 95), \"flow_rate\": (0.5, 6),\n",
    "#             \"slurry_concentration\": (1.5, 10), \"init_Ca\": (1000, 300000),\n",
    "#             \"init_K\": (300, 3000), \"init_Li\": (15000, 300000),\n",
    "#             \"init_Mg\": (10, 20000), \"init_Na\": (300, 20000)\n",
    "#         }\n",
    "#     l_bounds = [v[0] for v in bounds.values()]\n",
    "#     u_bounds = [v[1] for v in bounds.values()]\n",
    "#     num_initial_points = int(4 * n_points + 50) if lhs_sampler else int(4 * n_points + 10)\n",
    "#     sample = latin_hypercube_sample(num_initial_points, len(bounds), l_bounds, u_bounds, seed=seed)\n",
    "#     exp_grid = pd.DataFrame(sample, columns=list(bounds.keys()))\n",
    "#     exp_grid.loc[exp_grid['T_cold'] > exp_grid['T_hot'], ['T_cold', 'T_hot']] = exp_grid.loc[exp_grid['T_cold'] > exp_grid['T_hot'], ['T_hot', 'T_cold']].values\n",
    "#     exp_grid['T_hot'] += delta_T\n",
    "#     for parameter, (lower_bound, upper_bound) in bounds.items():\n",
    "#         exp_grid = exp_grid[exp_grid[parameter].between(lower_bound, upper_bound)]\n",
    "#     return exp_grid.sample(n=min(n_points, len(exp_grid)), random_state=seed).reset_index(drop=True)\n",
    "\n",
    "# def perform_full_analysis(data, feature_columns, target_columns, pair_plot=False):\n",
    "#     \"\"\"Conducts a full analysis including tuning, training, and various plots.\"\"\"\n",
    "#     for target in target_columns:\n",
    "#         print(f\"\\n--- Analyzing for Target: {target} ---\")\n",
    "#         y = data[target]\n",
    "#         X = data[feature_columns]\n",
    "#         model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "#         model.fit(X, y)\n",
    "        \n",
    "#         fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "#         # Pearson Correlation Matrix\n",
    "#         correlation_matrix = data[feature_columns + [target]].corr()\n",
    "#         sns.heatmap(correlation_matrix[[target]].sort_values(by=target, ascending=False), annot=True, cmap='coolwarm', ax=axs[0])\n",
    "#         axs[0].set_title(f'Pearson Correlation with {target}')\n",
    "        \n",
    "#         # SHAP Feature Importance\n",
    "#         explainer = shap.Explainer(model)\n",
    "#         shap_values = explainer(X)\n",
    "#         shap_sum = np.abs(shap_values.values).mean(axis=0)\n",
    "#         sorted_indices = np.argsort(shap_sum)\n",
    "#         axs[1].barh(X.columns[sorted_indices], shap_sum[sorted_indices])\n",
    "#         axs[1].set_title('SHAP Feature Importance')\n",
    "        \n",
    "#         # Sensitivity Analysis\n",
    "#         importances = {}\n",
    "#         baseline_mse = mean_squared_error(y, model.predict(X))\n",
    "#         for feature in feature_columns:\n",
    "#             X_perturbed = X.copy()\n",
    "#             X_perturbed[feature] += X_perturbed[feature].std()\n",
    "#             mse_perturbed = mean_squared_error(y, model.predict(X_perturbed))\n",
    "#             importances[feature] = abs(mse_perturbed - baseline_mse) / baseline_mse\n",
    "#         sorted_features = sorted(importances, key=importances.get)\n",
    "#         axs[2].barh(sorted_features, [importances[f] for f in sorted_features])\n",
    "#         axs[2].set_title('Normalized Sensitivity Analysis')\n",
    "        \n",
    "#         plt.tight_layout()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Data Loading with Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _standardize_missing(df: pd.DataFrame,\n",
    "                         extra_na: Optional[Iterable[Any]] = None) -> pd.DataFrame:\n",
    "    \"\"\"Unify all missing markers to Pandas NA and use nullable dtypes.\"\"\"\n",
    "    na_tokens = {\n",
    "        \"\", \" \", \"-\", \"—\", \"–\",\n",
    "        \"na\", \"n/a\", \"nan\", \"none\", \"null\",\n",
    "        \"NA\", \"N/A\", \"NaN\", \"None\", \"NULL\"\n",
    "    }\n",
    "    if extra_na:\n",
    "        na_tokens.update(extra_na)\n",
    "\n",
    "    def _strip_and_na(x):\n",
    "        if isinstance(x, str):\n",
    "            s = x.strip()\n",
    "            return pd.NA if s == \"\" else s\n",
    "        return x\n",
    "\n",
    "    df = df.applymap(_strip_and_na)\n",
    "    df = df.replace(list(na_tokens), pd.NA)\n",
    "    return df.convert_dtypes()\n",
    "\n",
    "\n",
    "def read_table_resilient(\n",
    "    file: Union[str, BytesIO],\n",
    "    sheet: Optional[Union[int, str]] = None,\n",
    "    prefer_first_sheet: bool = True,\n",
    "    csv_na_values: Optional[Iterable[str]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read an Excel-like file into a DataFrame with robust fallbacks and unified NA.\n",
    "\n",
    "    Attempts (in order):\n",
    "      1) pandas.read_excel (default)\n",
    "      2) pandas.read_excel(engine='openpyxl')\n",
    "      3) pandas.ExcelFile(..., engine='openpyxl').parse(sheet)\n",
    "      4) Manual openpyxl → DataFrame\n",
    "      5) openpyxl → in-memory CSV → pandas.read_csv\n",
    "      6) pandas.read_excel(engine='xlrd') [legacy .xls only]\n",
    "\n",
    "    Missing values are standardized to Pandas NA.\n",
    "\n",
    "    Args:\n",
    "        file: Path or BytesIO.\n",
    "        sheet: Sheet index or name; if None, tries first sheet.\n",
    "        prefer_first_sheet: If True and sheet is None, explicitly targets first sheet.\n",
    "        csv_na_values: Extra NA tokens for CSV-based fallbacks.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    is_path = isinstance(file, str)\n",
    "    file_for_open = file if is_path else BytesIO(file.getvalue())\n",
    "\n",
    "    # --- 1) Default pandas.read_excel ---\n",
    "    try:\n",
    "        df = pd.read_excel(file, sheet_name=sheet if sheet is not None else 0 if prefer_first_sheet else None)\n",
    "        return _standardize_missing(df, csv_na_values)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # --- 2) pandas.read_excel with openpyxl ---\n",
    "    try:\n",
    "        df = pd.read_excel(file, sheet_name=sheet if sheet is not None else 0 if prefer_first_sheet else None,\n",
    "                           engine=\"openpyxl\")\n",
    "        return _standardize_missing(df, csv_na_values)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # --- 3) pandas.ExcelFile(..., openpyxl).parse ---\n",
    "    try:\n",
    "        from pandas import ExcelFile\n",
    "        xls = ExcelFile(file, engine=\"openpyxl\")\n",
    "        target = sheet if sheet is not None else (xls.sheet_names[0] if prefer_first_sheet else 0)\n",
    "        df = xls.parse(sheet_name=target)\n",
    "        return _standardize_missing(df, csv_na_values)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # --- 4) Manual openpyxl → DataFrame ---\n",
    "    try:\n",
    "        import openpyxl\n",
    "        wb = openpyxl.load_workbook(file_for_open, read_only=True, data_only=True)\n",
    "        ws = wb[sheet] if isinstance(sheet, str) and sheet in wb.sheetnames else (\n",
    "            wb.worksheets[sheet] if isinstance(sheet, int) and 0 <= sheet < len(wb.worksheets) else wb.active\n",
    "        )\n",
    "        data = list(ws.values)\n",
    "        if not data:\n",
    "            raise ValueError(\"No data found in worksheet.\")\n",
    "        headers, rows = data[0], data[1:]\n",
    "        df = pd.DataFrame(rows, columns=headers)\n",
    "        return _standardize_missing(df, csv_na_values)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # --- 5) openpyxl → CSV → pandas.read_csv ---\n",
    "    try:\n",
    "        import openpyxl\n",
    "        wb = openpyxl.load_workbook(file_for_open, read_only=True, data_only=True)\n",
    "        ws = wb[sheet] if isinstance(sheet, str) and sheet in wb.sheetnames else (\n",
    "            wb.worksheets[sheet] if isinstance(sheet, int) and 0 <= sheet < len(wb.worksheets) else wb.active\n",
    "        )\n",
    "        buf = StringIO()\n",
    "        w = csv.writer(buf)\n",
    "        for row in ws.iter_rows(values_only=True):\n",
    "            w.writerow([\"\" if v is None else v for v in row])\n",
    "        buf.seek(0)\n",
    "        df = pd.read_csv(\n",
    "            buf,\n",
    "            na_values=set(csv_na_values or []),\n",
    "            keep_default_na=True,\n",
    "        )\n",
    "        return _standardize_missing(df, csv_na_values)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # --- 6) pandas.read_excel with xlrd (legacy .xls only) ---\n",
    "    try:\n",
    "        if is_path and os.path.splitext(file)[1].lower() == \".xls\":\n",
    "            df = pd.read_excel(file, sheet_name=sheet if sheet is not None else 0 if prefer_first_sheet else None,\n",
    "                               engine=\"xlrd\")\n",
    "            return _standardize_missing(df, csv_na_values)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    raise ValueError(\"All read attempts failed. Check file format or integrity.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation\n",
    "\n",
    "We begin by loading the complete experimental dataset from the supplementary Excel file. This file contains the results from all 80 experiments conducted throughout the study. The raw data is then cleaned and standardized using our `sift_data_extractor` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the path to the data file\n",
    "# Note: Please adjust the path if you are running this notebook in a different directory structure.\n",
    "file_path = \"Data/raw/2024_03_05_SIFT.xlsx\"\n",
    "\n",
    "# Read the excel file\n",
    "# raw_data = pd.read_excel(file_path)\n",
    "raw_data = read_table_resilient(file_path)\n",
    "\n",
    "# Extract and clean the data\n",
    "cleaned_data = sift_data_extractor(raw_data, ppm=True, slurry=True, score=True, NaOH=True)\n",
    "\n",
    "# Display the first few rows of the cleaned data\n",
    "print(\"Cleaned Data Head:\")\n",
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Filtering and Feature Engineering\n",
    "\n",
    "Next, we filter out any unsuccessful or anomalous experiments to ensure the quality of our training data. We then engineer two important features:\n",
    "\n",
    "1.  **$\\Delta T$**: The temperature difference between the hot and cold reactors ($T_{hot} - T_{cold}$), a critical process parameter.\n",
    "2.  **`bg` (Battery Grade)**: A binary label indicating whether an experiment achieved the battery-grade specification for magnesium (`fini_Mg` < 80 ppm). This will be our target for classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to include only successful experiments\n",
    "cleaned_data = successful_sift_extraction(cleaned_data).reset_index(drop=True)\n",
    "\n",
    "# Additional filtering to remove specific experiments with known issues\n",
    "experiments_to_drop = ['Sift-035', 'Sift-037', 'Sift-038', 'Sift-051', 'Sift-052', 'Sift-050', 'Sift-033', 'Sift-023', 'Sift-055', 'Sift-017', 'Sift-014', 'Sift-064', 'Sift_066', 'Sift-087','Sift-092']\n",
    "cleaned_data = cleaned_data[~cleaned_data['experiment_id'].isin(experiments_to_drop)].reset_index(drop=True)\n",
    "\n",
    "# Feature Engineering\n",
    "cleaned_data['delta_T'] = cleaned_data['T_hot'] - cleaned_data['T_cold']\n",
    "cleaned_data['bg'] = (cleaned_data['fini_Mg'] < 80).astype(int) # 0 for battery-grade, 1 for non-battery-grade\n",
    "\n",
    "print(f\"Final dataset contains {len(cleaned_data)} experiments.\")\n",
    "cleaned_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Progressive Data Loading\n",
    "\n",
    "To simulate the active learning cycles, we also load the data progressively, as it was collected over time. This creates a list of DataFrames, where each represents the cumulative data available at a specific stage of the HITL-AL process. This is essential for demonstrating the evolution of the model's understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Update the file paths below to match the location of your data files.\n",
    "data = pd.DataFrame() # Initialize empty dataframe\n",
    "data_by_date = []\n",
    "\n",
    "try:\n",
    "    file_path = \"Data/raw/2024_03_05_SIFT.xlsx\" #<-- UPDATE THIS PATH\n",
    "    data = pd.read_excel(file_path)\n",
    "    print(\"Final dataset loaded successfully.\")\n",
    "    print(data.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Final dataset not found at {file_path}. Please update the path.\")\n",
    "\n",
    "file_names = [\n",
    "    '2023_03_29_SIFT.xlsx', '2023_04_04_SIFT.xlsx', '2023_04_10_SIFT.xlsx',\n",
    "    '2023_04_12_SIFT.xlsx', '2023_05_02_SIFT.xlsx', '2023_05_18_SIFT.xlsx',\n",
    "    '2023_06_12_SIFT.xlsx', '2023_07_16_SIFT.xlsx', '2023_09_18_SIFT.xlsx',\n",
    "    '2023_10_31_SIFT.xlsx', '2023_12_13_SIFT.xlsx', '2024_03_05_SIFT.xlsx'\n",
    "]\n",
    "file_path_all = \"Data/raw/\" #<-- UPDATE THIS PATH\n",
    "\n",
    "for file in file_names:\n",
    "    try:\n",
    "        full_path = file_path_all + file\n",
    "        file_data = pd.read_excel(full_path)\n",
    "        data_by_date.append(file_data)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Progressive data file not found at {full_path}. Please update the path.\")\n",
    "        \n",
    "if data_by_date:\n",
    "    print(f\"\\nLoaded {len(data_by_date)} progressive datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Data Cleaning and Feature Engineering\n",
    "\n",
    "We use our custom functions to extract relevant columns, handle different units, and create a clean DataFrame. We then filter for only successful experiments and engineer new features that might be insightful, such as the temperature differential, $\\Delta T = T_{hot} - T_{cold}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    cleaned_data = sift_data_extractor(data, slurry=True, pH=False, score=True, NaOH=True)\n",
    "    if 'crystal formation rate (mg/h)' in data.columns:\n",
    "        cleaned_data['crystal formation rate (mg/h)'] = data['crystal formation rate (mg/h)'].copy()\n",
    "    \n",
    "    cleaned_data = successful_sift_extraction(cleaned_data).reset_index(drop=True)\n",
    "    \n",
    "    failed_experiments = [\n",
    "        'Sift-035', 'Sift-037', 'Sift-038', 'Sift-051', 'Sift-052', 'Sift-050',\n",
    "        'Sift-033', 'Sift-023', 'Sift-055', 'Sift-017', 'Sift-014', 'Sift-064',\n",
    "        'Sift_066', 'Sift-087', 'Sift-092'\n",
    "    ]\n",
    "    cleaned_data = cleaned_data[~cleaned_data['experiment_id'].isin(failed_experiments)].reset_index(drop=True)\n",
    "\n",
    "    cleaned_data['delta_T'] = cleaned_data['T_hot'] - cleaned_data['T_cold']\n",
    "    cleaned_data['delta_Mg'] = (cleaned_data['init_Mg'] - cleaned_data['fini_Mg']) / cleaned_data['init_Mg']\n",
    "    cleaned_data_all = cleaned_data.copy()\n",
    "    \n",
    "    # An outcome is battery-grade (bg=1) if final Mg concentration is < 80 ppm.\n",
    "    cleaned_data_all['bg'] = np.where(cleaned_data_all['fini_Mg'] < 80, 1, 0)\n",
    "    print(\"Final dataset cleaned and prepared.\")\n",
    "    print(f\"Total successful, valid experiments: {len(cleaned_data_all)}\")\n",
    "    print(cleaned_data_all.describe())\n",
    "\n",
    "cleaned_data_by_date = []\n",
    "if data_by_date:\n",
    "    for i, df in enumerate(data_by_date):\n",
    "        cleaned_data_single = sift_data_extractor(df, slurry=True, pH=False, score=False, NaOH=True)\n",
    "        cleaned_data_single['delta_T'] = cleaned_data_single['T_hot'] - cleaned_data_single['T_cold']\n",
    "        cleaned_data_single['bg'] = np.where(cleaned_data_single['fini_Mg'] < 80, 1, 0)\n",
    "        cols_to_keep = ['experiment_id', 'T_cold', 'T_hot', 'delta_T', 'flow_rate', 'slurry_concentration', 'init_Ca', 'init_K', 'init_Li', 'init_Mg', 'init_Na', 'fini_Ca', 'fini_K', 'fini_Li', 'fini_Mg', 'fini_Na', 'fini_Li_purity', 'bg']\n",
    "        cleaned_data_single = cleaned_data_single[[col for col in cols_to_keep if col in cleaned_data_single.columns]].copy()\n",
    "        cleaned_data_single.dropna(inplace=True)\n",
    "        cleaned_data_by_date.append(cleaned_data_single)\n",
    "    print(\"\\nProgressive datasets cleaned and prepared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data and Results Inspection\n",
    "\n",
    "This section corresponds to **Section 2.2.1** of the paper. We perform a full statistical analysis on our final cleaned dataset to understand the relationships between our input parameters (features) and key outcomes. A random feature is included as a baseline to gauge the significance of our actual process parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cleaned_data_all' in locals() and not cleaned_data_all.empty:\n",
    "    find_missing_data(cleaned_data_all)\n",
    "\n",
    "    feature_columns = ['T_cold', 'T_hot', 'delta_T', 'flow_rate', 'slurry_concentration', 'init_Ca', 'init_K', 'init_Li', 'init_Mg', 'init_Na']\n",
    "    target_columns_mg = ['fini_Mg']\n",
    "    analysis_df_mg = cleaned_data_all.copy()\n",
    "    analysis_df_mg['random'] = np.random.random(size=len(analysis_df_mg))\n",
    "    feature_columns_with_random = feature_columns + ['random']\n",
    "    \n",
    "    # Perform analysis for Final Mg Concentration\n",
    "    perform_full_analysis(analysis_df_mg.dropna(subset=feature_columns_with_random + target_columns_mg), \n",
    "                          feature_columns_with_random, \n",
    "                          target_columns_mg)\n",
    "\n",
    "    # Perform analysis for Battery Grade Label\n",
    "    target_columns_bg = ['bg']\n",
    "    analysis_df_bg = cleaned_data_all.copy().dropna(subset=feature_columns + target_columns_bg)\n",
    "    analysis_df_bg['random'] = np.random.random(size=len(analysis_df_bg))\n",
    "    perform_full_analysis(analysis_df_bg, feature_columns_with_random, target_columns_bg)\n",
    "else:\n",
    "    print(\"Skipping analysis because data was not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Inspections: Cycle by Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for features and targets\n",
    "feature_columns = ['T_cold', 'T_hot', 'delta_T', 'flow_rate', 'slurry_concentration', 'init_Ca', 'init_K', 'init_Li', 'init_Mg', 'init_Na']\n",
    "target_columns = ['fini_Mg', 'fini_K', 'fini_Li_purity', 'fini_Ca', 'fini_Na']\n",
    "\n",
    "# Create a copy of the DataFrame\n",
    "cleaned_data_all_copy = cleaned_data_by_date[0][['T_cold', 'T_hot', 'delta_T', 'flow_rate', 'slurry_concentration', 'init_Ca', 'init_K', 'init_Li', 'init_Mg', 'init_Na',\n",
    "                                                 'fini_Mg', 'fini_K', 'fini_Li_purity', 'fini_Ca', 'fini_Na']].copy()\n",
    "\n",
    "# Add a random feature column\n",
    "np.random.seed(42)  # for reproducibility\n",
    "cleaned_data_all_copy['random'] = np.random.random(size=len(cleaned_data_all_copy))\n",
    "\n",
    "# Include the random column in the feature columns list\n",
    "feature_columns.append('random')\n",
    "\n",
    "# Perform analysis for each target\n",
    "perform_full_analysis(cleaned_data_all_copy, feature_columns, target_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploration Phase: Pareto Frontier Analysis\n",
    "\n",
    "The exploration phase of our HITL-AL framework aims to efficiently search the vast parameter space to find regions that are promising for achieving our objectives. The primary goal here is to minimize the final concentrations of both Magnesium (`fini_Mg`) and Calcium (`fini_Ca`), the two most challenging impurities.\n",
    "\n",
    "To do this, we employ a two-step process in each active learning cycle:\n",
    "1.  **Gaussian Process Regression (GPR)**: We train a GPR model on the accumulated experimental data. This model learns the relationship between the input parameters (temperatures, flow rates, initial concentrations) and the output impurities. The GPR provides not only a prediction but also an uncertainty estimate, which is crucial for exploration.\n",
    "2.  **Pareto Frontier Extraction (NSGA-II)**: Using the trained GPR model, we predict the outcomes for a large surrogate space of potential experiments (10,000 points generated via Latin Hypercube Sampling). We then use the NSGA-II genetic algorithm to identify the *Pareto frontier*—a set of experimental conditions that represent the optimal trade-offs for minimizing both `fini_Mg` and `fini_Ca` simultaneously. A point is on the Pareto front if you cannot improve one objective without worsening the other.\n",
    "\n",
    "The following cells demonstrate this process for a single cycle and then visualize the evolution of the Pareto frontier across multiple cycles, showing how the model's understanding improves over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Single Cycle Pareto Frontier Extraction Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the experimental data captured on 2023-04-04\n",
    "file_path = \"Data\\\\clean\\\\2023_04_04_SIFT_cleaned.csv\"\n",
    "data_2023_04_04 = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# Add temperature difference:\n",
    "data_2023_04_04['delta_T'] = data_2023_04_04['T_hot'] - data_2023_04_04['T_cold'] \n",
    "\n",
    "# Clean the data based on the recipe used for 2023-04-04 (in active learning projects your recipe changes from cycle to cycle) \n",
    "## Drop unnecessary columns\n",
    "cleaned_data_2023_04_04 = data_2023_04_04.drop(['init_Li_purity', 'fini_Li_purity', 'init_B',\n",
    "                                                'init_Si', 'fini_B', 'fini_Si', 'init_Sr', 'fini_Sr',\n",
    "                                                'scl_Ca', 'scl_K', 'scl_Li', 'scl_Mg', 'scl_Na', 'scl_Sr'], axis=1)\n",
    "cleaned_data_2023_04_04 = cleaned_data_2023_04_04.dropna()\n",
    "\n",
    "# Load the experimental grid used for analysing that data \n",
    "exp_grid_norm = pd.read_csv('Data\\\\generated\\\\Sift_lhs_10000_2023_04_04.csv')\n",
    "exp_grid_norm['delta_T'] = exp_grid_norm['T_hot'] - exp_grid_norm[\"T_cold\"]\n",
    "\n",
    "# Define the training and testing data\n",
    "X_train = cleaned_data_2023_04_04[['T_cold', 'T_hot', 'delta_T', 'flow_rate', 'slurry_concentration','init_Ca',\n",
    "                                 'init_K', 'init_Li', 'init_Mg', 'init_Na']].copy()\n",
    "\n",
    "y_train = cleaned_data_2023_04_04[['fini_Ca', 'fini_K', 'fini_Li', 'fini_Mg', 'fini_Na']].copy()\n",
    "\n",
    "X_test = exp_grid_norm[['T_cold', 'T_hot', 'delta_T', 'flow_rate', 'slurry_concentration','init_Ca',\n",
    "                        'init_K', 'init_Li', 'init_Mg', 'init_Na']].copy()\n",
    "\n",
    "scale = StandardScaler()\n",
    "scaler = scale.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# GPR prediction\n",
    "gpr_flexible = GaussianProcessRegressor(Matern(length_scale=1, nu=1.5), \n",
    "                                        alpha=1e-10,\n",
    "                                        random_state = 0,\n",
    "                                        n_restarts_optimizer=10)\n",
    "\n",
    "gpr_flexible.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred, y_std = gpr_flexible.predict(X_test_scaled, return_std=True)\n",
    "\n",
    "gpr_pred_df = pd.DataFrame(y_pred, columns=['fini_Ca', 'fini_K', 'fini_Li', 'fini_Mg', 'fini_Na'])\n",
    "\n",
    "grid_gpr = pd.concat([exp_grid_norm, gpr_pred_df], axis=1)\n",
    "grid_gpr.head()\n",
    "\n",
    "\n",
    "# create a new figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# scatter plot with log-scale axes\n",
    "scatter = ax.scatter(gpr_pred_df['fini_Mg'], gpr_pred_df['fini_Ca'])\n",
    "ax.set_xscale('linear')\n",
    "ax.set_yscale('linear')\n",
    "\n",
    "# add colorbar and labels\n",
    "# clb = plt.colorbar(scatter)\n",
    "# clb.ax.set_ylabel('fini_Ca')\n",
    "plt.xlabel('fini_Mg')\n",
    "plt.ylabel('fini_Ca')\n",
    "\n",
    "\n",
    "# plot final Mg threshold: \n",
    "\n",
    "plt.scatter(y_train['fini_Mg'], y_train['fini_Ca'], c='r', label='training data')\n",
    "\n",
    "# set the legend and tighten the layout\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function call optimizes the Pareto front based on impurity levels\n",
    "# of magnesium (fini_Mg) and calcium (fini_Ca) using the NSGA-II algorithm.\n",
    "# It returns two dataframes:\n",
    "# 1. pareto_front_df – objective-only Pareto front\n",
    "# 2. grid_gpr_pareto_df – full rows of grid_gpr corresponding to selected Pareto indices\n",
    "\n",
    "pareto_front_df, grid_gpr_pareto_df = optimize_pareto_front(\n",
    "    grid_gpr,              # Input dataframe with impurity predictions\n",
    "    obj1_col=\"fini_Mg\",    # First objective column\n",
    "    obj2_col=\"fini_Ca\",    # Second objective column\n",
    "    min_unique_points=30,  # Minimum number of unique Pareto points to collect\n",
    "    population_size=500,   # Population size for NSGA-II algorithm\n",
    "    generations=10000      # Number of generations to run per cycle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manual Replication of Optimization Workflow without Function\n",
    "\n",
    "# # Define the objective function to extract fini_Mg and fini_Ca for a given row index\n",
    "# def objective(x):\n",
    "#     idx = [int(i) for i in x]  # Ensure index is integer\n",
    "#     f1 = grid_gpr.loc[idx, 'fini_Mg'].values\n",
    "#     f2 = grid_gpr.loc[idx, 'fini_Ca'].values\n",
    "#     return [f1[0], f2[0]]\n",
    "\n",
    "# # Define the optimization problem: 1 decision variable (row index), 2 objectives\n",
    "# problem = Problem(1, 2)\n",
    "# problem.types[:] = Real(0, grid_gpr.shape[0] - 1)\n",
    "# problem.function = objective\n",
    "\n",
    "# # Prepare a simplified DataFrame with just the two objective columns\n",
    "# df = grid_gpr[['fini_Mg', 'fini_Ca']].rename(columns={\n",
    "#     'fini_Mg': 'obj1',\n",
    "#     'fini_Ca': 'obj2'\n",
    "# })\n",
    "\n",
    "# # Initialize NSGA-II with a specified population size\n",
    "# algorithm = NSGAII(problem, population_size=500)\n",
    "\n",
    "# # Track unique Pareto points and their indices\n",
    "# pareto_front_unique = pd.DataFrame()\n",
    "# pareto_indices_unique = set()\n",
    "\n",
    "# # Iterate until enough unique Pareto points are found\n",
    "# while len(pareto_front_unique) < 30:\n",
    "#     # Run the optimization algorithm\n",
    "#     algorithm.run(10000)\n",
    "\n",
    "#     # Extract the current nondominated (Pareto) solutions\n",
    "#     front = nondominated(algorithm.result)\n",
    "\n",
    "#     # Extract corresponding indices of the Pareto front\n",
    "#     pareto_indices = [int(solution.variables[0]) for solution in front]\n",
    "#     pareto_indices_unique.update(pareto_indices)\n",
    "\n",
    "#     # Add the new Pareto rows to the cumulative dataframe\n",
    "#     pareto_front = df.iloc[pareto_indices]\n",
    "#     pareto_front_unique = pareto_front_unique.append(pareto_front, ignore_index=False)\n",
    "#     pareto_front_unique = pareto_front_unique.drop_duplicates()\n",
    "\n",
    "#     # If too few unique rows are found, reinitialize the optimizer and retry\n",
    "#     if len(pareto_front_unique) < 30:\n",
    "#         print(\"Warning: Found fewer than 30 unique Pareto points. Retrying NSGA-II.\")\n",
    "#         print(f\"Current Pareto front length: {len(pareto_front_unique)}\")\n",
    "#         algorithm = NSGAII(problem, population_size=500)\n",
    "\n",
    "\n",
    "# # Final Output and Display\n",
    "\n",
    "# # Display the unique Pareto front (objective values only)\n",
    "# print(\"Unique Pareto front (first 30 rows):\")\n",
    "# print(pareto_front_unique.head(30))\n",
    "\n",
    "# # Display full rows of grid_gpr corresponding to unique Pareto indices\n",
    "# print(\"\\nFull unique rows from grid_gpr:\")\n",
    "# grid_gpr_unique = grid_gpr.iloc[list(pareto_indices_unique)].drop_duplicates()\n",
    "# print(grid_gpr_unique.head(30))ront_df, grid_gpr_pareto_df = optimize_pareto_front(\n",
    "#     grid_gpr,\n",
    "#     obj1_col=\"fini_Mg\",\n",
    "#     obj2_col=\"fini_Ca\",\n",
    "#     min_unique_points=30,\n",
    "#     population_size=500,\n",
    "#     generations=10000\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import ticker\n",
    "\n",
    "# create a new figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# scatter plot with log-scale axes\n",
    "ax.scatter(grid_gpr['fini_Mg'], grid_gpr['fini_Ca'])\n",
    "ax.scatter(grid_gpr_pareto_df['fini_Mg'], grid_gpr_pareto_df['fini_Ca'], c='cyan', label = 'NSGA-II Pareto frontier')\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('fini_Mg')\n",
    "plt.ylabel('fini_Ca')\n",
    "\n",
    "\n",
    "# set the legend and tighten the layout\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Evolution of Pareto Frontiers\n",
    "\n",
    "The plot below visualizes the GPR-predicted Pareto frontiers from four different cycles of the active learning process. As more experimental data is gathered, the GPR model becomes more accurate, and the predicted Pareto front shifts towards the bottom-left corner, representing improved (lower) impurity concentrations. This demonstrates the progressive optimization achieved by the HITL-AL framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_files = {\n",
    "    'Cycle 1': 'Data/generated/AI_suggestions_pareto_2023_03_31.csv',\n",
    "    'Cycle 2': 'Data/generated/AI_suggestions_pareto_2023_04_04.csv',\n",
    "    'Cycle 3': 'Data/generated/AI_suggestions_pareto_2023_04_10.csv',\n",
    "    'Cycle 4': 'Data/generated/AI_suggestions_pareto_2023_04_24.csv'\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "colors = plt.cm.plasma(np.linspace(0, 1, len(pareto_files)))\n",
    "\n",
    "for i, (cycle_name, file_path) in enumerate(pareto_files.items()):\n",
    "    pareto_df = pd.read_csv(file_path)\n",
    "    plt.scatter(pareto_df['fini_Mg'], pareto_df['fini_Ca'], color=colors[i], label=cycle_name, s=50, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Predicted Final Mg (ppm)')\n",
    "plt.ylabel('Predicted Final Ca (ppm)')\n",
    "plt.title('Evolution of Pareto Frontiers Over Active Learning Cycles')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1. Cycle by Cycle Patero Evolution Demonstratoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_03_31 = pd.read_csv('Data/generated/AI_suggestions_pareto_2023_03_31.csv')\n",
    "pareto_04_04 = pd.read_csv('Data/generated/AI_suggestions_pareto_2023_04_04.csv')\n",
    "pareto_04_10 = pd.read_csv('Data/generated/AI_suggestions_pareto_2023_04_10.csv')\n",
    "pareto_04_24 = pd.read_csv('Data/generated/AI_suggestions_pareto_2023_04_24.csv')\n",
    "pareto_05_04 = pd.read_csv('Data/generated/AI_suggestions_pareto_2023_05_04.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Data/clean/2023_03_29_SIFT.csv'\n",
    "file_path_lhs = 'Data/generated/Sift_lhs_10000_2023_03_31.csv'\n",
    "\n",
    "grid_gpr, X_train, y_train = pareto_gpr_prediction(file_path, file_path_lhs, multi_dimensional = True, delta_T=False, alpha = 1e-5)\n",
    "\n",
    "print(grid_gpr.shape)\n",
    "\n",
    "# create a new figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# scatter plot with log-scale axes\n",
    "scatter = ax.scatter(grid_gpr['fini_Mg'], grid_gpr['fini_Ca'], label = 'GPR')\n",
    "ax.set_xscale('linear')\n",
    "ax.set_yscale('linear')\n",
    "\n",
    "plt.xlabel('fini_Mg')\n",
    "plt.ylabel('fini_Ca')\n",
    "\n",
    "plt.scatter(y_train['fini_Mg'], y_train['fini_Ca'], c='r', label='training data')\n",
    "\n",
    "# set the legend and tighten the layout\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.scatter(pareto_03_31['fini_Mg'], pareto_03_31['fini_Ca'], label='pareto_03_31')\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Data\\\\clean\\\\2023_04_04_SIFT_cleaned.csv\"\n",
    "file_path_lhs = 'Data\\\\generated\\\\Sift_lhs_10000_2023_04_04.csv'\n",
    "\n",
    "grid_gpr, X_train, y_train = pareto_gpr_prediction(file_path, file_path_lhs, multi_dimensional = True)\n",
    "\n",
    "\n",
    "print(grid_gpr.shape)\n",
    "\n",
    "# create a new figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# scatter plot with log-scale axes\n",
    "scatter = ax.scatter(grid_gpr['fini_Mg'], grid_gpr['fini_Ca'], label = 'GPR')\n",
    "ax.set_xscale('linear')\n",
    "ax.set_yscale('linear')\n",
    "\n",
    "plt.xlabel('fini_Mg')\n",
    "plt.ylabel('fini_Ca')\n",
    "\n",
    "plt.scatter(y_train['fini_Mg'], y_train['fini_Ca'], c='r', label='training data')\n",
    "\n",
    "# set the legend and tighten the layout\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.scatter(pareto_04_04['fini_Mg'], pareto_04_04['fini_Ca'], label='pareto_04_04')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Data\\\\clean\\\\2023_04_10_SIFT_cleaned.csv\"\n",
    "file_path_lhs = 'Data\\\\generated\\\\Sift_lhs_10000_2023_04_04.csv'\n",
    "\n",
    "grid_gpr, X_train, y_train = pareto_gpr_prediction(file_path, file_path_lhs, multi_dimensional = True)\n",
    "\n",
    "\n",
    "print(grid_gpr.shape)\n",
    "\n",
    "# create a new figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# scatter plot with log-scale axes\n",
    "scatter = ax.scatter(grid_gpr['fini_Mg'], grid_gpr['fini_Ca'], label = 'GPR')\n",
    "ax.set_xscale('linear')\n",
    "ax.set_yscale('linear')\n",
    "\n",
    "plt.xlabel('fini_Mg')\n",
    "plt.ylabel('fini_Ca')\n",
    "\n",
    "plt.scatter(y_train['fini_Mg'], y_train['fini_Ca'], c='r', label='training data')\n",
    "\n",
    "# set the legend and tighten the layout\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.scatter(pareto_04_10['fini_Mg'], pareto_04_10['fini_Ca'], label='pareto_04_10')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Data\\\\clean\\\\2023_04_12_SIFT_cleaned.csv\"\n",
    "file_path_lhs = 'Data\\\\generated\\\\Sift_lhs_10000_2023_04_04.csv'\n",
    "\n",
    "grid_gpr,  X_train, y_train  = pareto_gpr_prediction(file_path, file_path_lhs, multi_dimensional = False, alpha = 1e-5)\n",
    "\n",
    "\n",
    "print(grid_gpr.shape)\n",
    "\n",
    "# create a new figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# scatter plot with log-scale axes\n",
    "scatter = ax.scatter(grid_gpr['fini_Mg'], grid_gpr['fini_Ca'], label = 'GPR')\n",
    "ax.set_xscale('linear')\n",
    "ax.set_yscale('linear')\n",
    "\n",
    "plt.xlabel('fini_Mg')\n",
    "plt.ylabel('fini_Ca')\n",
    "\n",
    "plt.scatter(y_train['fini_Mg'], y_train['fini_Ca'], c='r', label='training data')\n",
    "\n",
    "# set the legend and tighten the layout\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.scatter(pareto_04_24['fini_Mg'], pareto_04_24['fini_Ca'], label='pareto_04_24')\n",
    "\n",
    "\n",
    "# ax.plot([ppm_threshold('fini_Mg'), ppm_threshold('fini_Mg')], [-50,350] , c='k', label='fini_Mg threshold')\n",
    "# ax.plot([-50,12500], [ppm_threshold('fini_Ca'), ppm_threshold('fini_Ca')], c='g', label='fini_Ca threshold')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The \"Aha!\" Moment: The Random Walk and the Role of $T_{cold}$\n",
    "\n",
    "Despite progress, reducing Mg impurities below the battery-grade threshold remained a challenge. This led to a key human intervention: questioning the initial assumptions about the parameter space. Experts hypothesized that the model might be stuck in a local minimum and that unexplored regions could hold the key.\n",
    "\n",
    "To test this, a **random walk algorithm** was deployed. This algorithm generated a new surrogate space of 5,000 points by taking small, random steps (±25%) from the boundaries of the last identified Pareto front. A GPR model, trained on all 36 available experiments, was then used to make predictions on this new \"wobbled\" space.\n",
    "\n",
    "The analysis of these predictions revealed a surprising and statistically significant inverse correlation between the cold reactor's temperature ($T_{cold}$) and the final Mg concentration. This contradicted prevailing heuristics and prior literature. The plots below replicate the analysis from Figure 4 in the paper, clearly showing this crucial insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-generated wobbled pareto data\n",
    "wobbled_pareto_data = pd.read_csv('Data/generated/wobbled_pareto_2023_04_24.csv')\n",
    "\n",
    "# Define features and target\n",
    "feature_columns = ['T_cold', 'T_hot', 'delta_T', 'flow_rate', 'slurry_concentration', 'init_Ca', 'init_K', 'init_Li', 'init_Mg', 'init_Na']\n",
    "target_columns = ['fini_Mg']\n",
    "\n",
    "# Add a random feature for baseline comparison\n",
    "wobbled_pareto_data['random'] = np.random.random(size=len(wobbled_pareto_data))\n",
    "feature_columns_with_random = feature_columns + ['random']\n",
    "\n",
    "# Perform analysis on the GPR predictions from the wobbled data\n",
    "print(\"Analysis of GPR Predictions on Wobbled Pareto Frontier Data\")\n",
    "perform_full_analysis(wobbled_pareto_data, feature_columns_with_random, target_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wobbled_pareto_2023_04_24 = pd.read_csv('Data\\\\generated\\\\wobbled_pareto_2023_04_24.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot pearson correlatino matrix: \n",
    "plt.figure(figsize=(12, 8))\n",
    "# Use fmt='.2f' to format the annotation to 2 decimal places\n",
    "sns.heatmap(wobbled_pareto_2023_04_24.corr(), annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title('Pearson Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analyses above clearly demonstrate that `T_cold` has the strongest negative correlation and highest feature importance for reducing `fini_Mg`, an insight that was not apparent from the initial data but was uncovered through this human-guided exploration.\n",
    "\n",
    "### 5.1. Experimental Validation\n",
    "\n",
    "Prompted by this model-derived insight, human experts designed a validation experiment to directly test the hypothesis. Two experiments were run with identical initial conditions, but one with a low $T_{cold}$ (10°C) and one with a high $T_{cold}$ (68°C). The results, mirroring Figure 5 in the paper, dramatically confirmed the model's prediction: the higher cold reactor temperature led to a remarkable reduction in Mg impurities, achieving the battery-grade target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar Plets: One by One Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Define the elements to compare\n",
    "elements = ['Ca', 'Mg', 'K', 'Na'] # 'Li', 'Mg', \n",
    "\n",
    "# Select the rows for 'Sift-099' and 'Sift-100'\n",
    "comparison_row_1 = cleaned_data[cleaned_data['experiment_id'] == 'Sift-099']\n",
    "comparison_row_2 = cleaned_data[cleaned_data['experiment_id'] == 'Sift-100']\n",
    "\n",
    "# Define distinct colors for initial and final values\n",
    "color_init = 'green'\n",
    "color_fini_1 = 'blue'\n",
    "color_fini_2 = 'maroon'\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(1, len(elements), figsize=(25, 8))\n",
    "\n",
    "# Plot each element separately\n",
    "for i, element in enumerate(elements):\n",
    "    ax = axs[i]\n",
    "\n",
    "    # Get the initial and final values for the current element from both rows\n",
    "    init_values_1 = comparison_row_1[f'init_{element}'].values[0]\n",
    "    fini_values_1 = comparison_row_1[f'fini_{element}'].values[0]\n",
    "    fini_values_2 = comparison_row_2[f'fini_{element}'].values[0]\n",
    "\n",
    "    # Calculate the percentage of the removed element\n",
    "    removed_percentage_1 = ((init_values_1 - fini_values_1) / init_values_1) * 100\n",
    "    removed_percentage_2 = ((init_values_1 - fini_values_2) / init_values_1) * 100\n",
    "\n",
    "    # Plot bars for initial and final values with distinct colors\n",
    "    ax.bar([0], [init_values_1], color=color_init, label=f'Initial')\n",
    "    ax.bar([1], [fini_values_1], color=color_fini_1, label=f'Sift-099 Final')\n",
    "    ax.bar([2], [fini_values_2], color=color_fini_2, label=f'Sift-100 Final')\n",
    "\n",
    "    # Add ppm notation on top of each bar\n",
    "    ax.text(0, init_values_1 + init_values_1/100, f'{init_values_1} ppm', ha='center', fontsize=12)\n",
    "    ax.text(1, fini_values_1 + init_values_1/100, f'{fini_values_1} ppm', ha='center', fontsize=12)\n",
    "    ax.text(2, fini_values_2 + init_values_1/100, f'{fini_values_2} ppm', ha='center', fontsize=12)\n",
    "    \n",
    "\n",
    "    # Add annotations for the removed element percentage\n",
    "    ax.text(1, fini_values_1 + init_values_1/25, f'{removed_percentage_1:.2f}%', ha='center', fontsize=12, color='blue')\n",
    "    ax.text(2, fini_values_2 + init_values_1/25, f'{removed_percentage_2:.2f}%', ha='center', fontsize=12, color='maroon')\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xticks([0, 1, 2])\n",
    "    ax.set_xticklabels(['Initial', 'Sift-099 Final', 'Sift-100 Final'], rotation=45)\n",
    "    ax.set_title(f'Comparison of Initial vs Final Values for {element}')\n",
    "    ax.set_ylabel('Values (ppm)')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Print all the ppm values and removed_percentages:\n",
    "    print('init: ', init_values_1)\n",
    "    print('fini_1: ', fini_values_1)\n",
    "    print('%_1: ', removed_percentage_1)\n",
    "    \n",
    "    print('fini_2: ', fini_values_2)\n",
    "    print('%_2: ', removed_percentage_2)\n",
    "    \n",
    "    # Print T_cold and T_hot values\n",
    "    print('T_cold:', comparison_row_1['T_cold'].values[0])\n",
    "    print('T_hot:', comparison_row_1['T_hot'].values[0])\n",
    "    \n",
    "    # Now for the comparison_row_2\n",
    "    print('T_cold:', comparison_row_2['T_cold'].values[0])\n",
    "    print('T_hot:', comparison_row_2['T_hot'].values[0])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments #38 and #39 from Table S4\n",
    "exp_high_T_cold = cleaned_data.iloc[37]\n",
    "exp_low_T_cold = cleaned_data.iloc[38]\n",
    "\n",
    "bar_labels = ['Initial', f\"Final (T_cold={exp_low_T_cold['T_cold']:.0f}°C)\", f\"Final (T_cold={exp_high_T_cold['T_cold']:.0f}°C)\"]\n",
    "mg_values = [exp_high_T_cold['init_Mg'], exp_low_T_cold['fini_Mg'], exp_high_T_cold['fini_Mg']]\n",
    "ca_values = [exp_high_T_cold['init_Ca'], exp_low_T_cold['fini_Ca'], exp_high_T_cold['fini_Ca']]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Magnesium plot\n",
    "axs[0].bar(bar_labels, mg_values, color=['gray', 'lightcoral', 'skyblue'])\n",
    "axs[0].axhline(y=ppm_threshold('fini_Mg'), color='r', linestyle='--', label='Mg Battery Grade Threshold (80 ppm)')\n",
    "axs[0].set_ylabel('Concentration (ppm)')\n",
    "axs[0].set_title('Impact of $T_{cold}$ on Final Mg Concentration')\n",
    "axs[0].legend()\n",
    "\n",
    "# Calcium plot\n",
    "axs[1].bar(bar_labels, ca_values, color=['gray', 'lightcoral', 'skyblue'])\n",
    "axs[1].axhline(y=ppm_threshold('fini_Ca'), color='r', linestyle='--', label='Ca Battery Grade Threshold (160 ppm)')\n",
    "axs[1].set_ylabel('Concentration (ppm)')\n",
    "axs[1].set_title('Impact of $T_{cold}$ on Final Ca Concentration')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KDE Plots: Distribution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to plot\n",
    "columns = ['T_cold', 'T_hot', 'delta_T', 'flow_rate', 'slurry_concentration',\n",
    "           'init_Ca', 'init_K', 'init_Li', 'init_Mg', 'init_Na']\n",
    "\n",
    "X_train_kde_plot = cleaned_data_all[['T_cold', 'T_hot', 'delta_T', 'flow_rate', 'slurry_concentration',\n",
    "           'init_Ca', 'init_K', 'init_Li', 'init_Mg', 'init_Na']].copy()\n",
    "y_train_kde_plot = cleaned_data_all[['fini_Mg', 'fini_Ca', 'fini_K', 'fini_Li', 'fini_Na', 'bg']].copy()   \n",
    "new_x_train = X_train_kde_plot.copy()\n",
    "new_y_train = y_train_kde_plot.copy()\n",
    "new_y_train['bg'] = new_y_train['bg'] = new_y_train['fini_Mg'] < 80\n",
    "new_data_df = pd.concat([new_x_train, new_y_train], axis=1)\n",
    "new_data_df_bg = new_data_df[new_data_df['bg']==True]\n",
    "new_data_df_nonbg = new_data_df[new_data_df['bg']==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All Data: No Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to plot for distribution analysis\n",
    "columns = ['T_cold', 'T_hot', 'delta_T', 'flow_rate', 'slurry_concentration',\n",
    "           'init_Ca', 'init_K', 'init_Li', 'init_Mg', 'init_Na']\n",
    "\n",
    "# Create subplots with 2 rows and 5 columns for better visualization\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20, 20))\n",
    "\n",
    "# Iterate over each column and plot its distribution in the corresponding subplot\n",
    "for i, column in enumerate(columns):\n",
    "    # Determine the row and column index for subplot positioning\n",
    "    row = i // 5  # Integer division to get row index (0 or 1)\n",
    "    col = i % 5   # Modulo operation to get column index (0 to 4)\n",
    "    ax = axes[row, col]  # Get the corresponding axis\n",
    "    \n",
    "    # Plot the KDE plot for the battery grade data (new_data_df_bg).\n",
    "    # No bounds are applied; the entire dataset is used.\n",
    "    sns.kdeplot(new_data_df_bg[column], color='blue', alpha=0.5, ax=ax, label='Battery Grade')\n",
    "    \n",
    "    # Plot the KDE plot for the non-battery grade data (new_data_df_nonbg).\n",
    "    # No bounds are applied; the entire dataset is used.\n",
    "    sns.kdeplot(new_data_df_nonbg[column], color='orange', alpha=0.5, ax=ax, label='Non-Battery Grade')\n",
    "    \n",
    "    # Set plot properties for title, labels, and legend\n",
    "    ax.set_title(f\"Distribution of {column}\")\n",
    "    ax.set_xlabel(column)\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend()\n",
    "\n",
    "# Adjust spacing between subplots to prevent overlapping titles and labels\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial Mg Within Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to plot for distribution analysis\n",
    "columns = ['T_cold', 'T_hot', 'delta_T', 'flow_rate', 'slurry_concentration',\n",
    "           'init_Ca', 'init_K', 'init_Li', 'init_Mg', 'init_Na']\n",
    "\n",
    "# Create subplots with 2 rows and 5 columns for better visualization\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20, 20))\n",
    "\n",
    "# Iterate over each column and plot in the corresponding subplot\n",
    "for i, column in enumerate(columns):\n",
    "    # Determine the row and column index for subplot positioning\n",
    "    row = i // 5  # Integer division to get row index (0 or 1)\n",
    "    col = i % 5   # Modulo operation to get column index (0 to 4)\n",
    "    ax = axes[row, col]  # Get the corresponding axis\n",
    "    \n",
    "    # Plot the KDE plot for battery grade data (BG=True) with filtering conditions:\n",
    "    # - init_Mg must be below 12000 (upper bound)\n",
    "    # - init_Mg must be above 200 (lower bound)\n",
    "    filtered_bg = new_data_df_bg[\n",
    "        (new_data_df_bg['init_Mg'] < 12000) & \n",
    "        (new_data_df_bg['init_Mg'] > 80)\n",
    "    ]\n",
    "    sns.kdeplot(filtered_bg[column], color='blue', alpha=0.5, ax=ax, label='Battery Grade (200 < init_Mg < 12000)')\n",
    "    \n",
    "    # Plot the KDE plot for non-battery grade data with the same filtering conditions:\n",
    "    # - init_Mg must be below 12000 (upper bound)\n",
    "    # - init_Mg must be above 200 (lower bound)\n",
    "    filtered_nonbg = new_data_df_nonbg[\n",
    "        (new_data_df_nonbg['init_Mg'] < 12000) & \n",
    "        (new_data_df_nonbg['init_Mg'] > 80)\n",
    "    ]\n",
    "    sns.kdeplot(filtered_nonbg[column], color='orange', alpha=0.5, ax=ax, label='Non-Battery Grade (200 < init_Mg < 12000)')\n",
    "    \n",
    "    # Set plot properties\n",
    "    ax.set_title(f\"Distribution of {column}\")\n",
    "    ax.set_xlabel(column)\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend()\n",
    "\n",
    "# Adjust spacing between subplots for better layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploitation Phase: Decision Boundary Refinement\n",
    "\n",
    "With the critical role of $T_{cold}$ validated, the strategy shifted from exploration to **exploitation**. The new objective was to precisely map the decision boundary between battery-grade and non-battery-grade outcomes, focusing on the interplay between initial Mg concentration and cold reactor temperature.\n",
    "\n",
    "### 6.1. Gaussian Process Classification (GPC)\n",
    "\n",
    "As detailed in the paper's methods section and Table S6, we employed a **Gaussian Process Classifier (GPC)** for this task. Unlike GPR, which predicts a continuous value, GPC predicts the probability of a data point belonging to a specific class (in this case, `bg=0` for battery-grade). We used a sophisticated kernel (`ConstantKernel * (Matern + WhiteKernel)`) to capture the complex, non-linear boundary.\n",
    "\n",
    "The plot below (replicating Figure 7) shows the GPC model's predictions projected onto the `init_Mg` vs. `T_cold` plane. The red dashed line represents the decision boundary (50% probability), effectively separating the parameter space into predicted battery-grade (purple) and non-battery-grade (yellow) regions. The experimental data points are overlaid, showing a strong agreement between the model and observed reality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['bg'] = np.where(cleaned_data['fini_Mg'] < 80, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare training data for classification\n",
    "X_train_cls = cleaned_data[['T_cold', 'init_Mg']].copy()\n",
    "y_train_cls = cleaned_data['bg'].copy()\n",
    "\n",
    "# Scale the features\n",
    "scaler_cls = StandardScaler().fit(X_train_cls)\n",
    "X_train_cls_scaled = scaler_cls.transform(X_train_cls)\n",
    "\n",
    "# Define the complex kernel for GPC\n",
    "kernel_gpc = ConstantKernel(1.0) * (Matern(length_scale=[0.3, 0.3], nu=1.5) + WhiteKernel(noise_level=0.06))\n",
    "\n",
    "# Train GPC model\n",
    "gpc_model = GaussianProcessClassifier(kernel=kernel_gpc, n_restarts_optimizer=10, max_iter_predict=100, random_state=42)\n",
    "gpc_model.fit(X_train_cls_scaled, y_train_cls)\n",
    "\n",
    "# Create 2D mesh for plotting the decision boundary\n",
    "T_cold_range = np.linspace(X_train_cls['T_cold'].min(), X_train_cls['T_cold'].max(), 100)\n",
    "init_Mg_range = np.linspace(0, 12000, 100)\n",
    "T_cold_mesh, init_Mg_mesh = np.meshgrid(T_cold_range, init_Mg_range)\n",
    "mesh_data = np.vstack([T_cold_mesh.ravel(), init_Mg_mesh.ravel()]).T\n",
    "mesh_data_scaled = scaler_cls.transform(mesh_data)\n",
    "\n",
    "# Predict probabilities on the mesh\n",
    "prob_mesh = gpc_model.predict_proba(mesh_data_scaled)[:, 1].reshape(T_cold_mesh.shape)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "contour = plt.contourf(T_cold_mesh, init_Mg_mesh, prob_mesh, cmap='viridis', alpha=0.7, levels=np.linspace(0, 1, 11))\n",
    "plt.colorbar(contour, label='Probability of Non-Battery Grade')\n",
    "plt.contour(T_cold_mesh, init_Mg_mesh, prob_mesh, levels=[0.5], colors='red', linestyles='--', linewidths=2, label='Decision Boundary')\n",
    "\n",
    "# Plot battery grade and non-battery grade separately for legend clarity\n",
    "battery_mask = y_train_cls == 0\n",
    "non_battery_mask = y_train_cls == 1\n",
    "plt.scatter(\n",
    "    X_train_cls.loc[battery_mask, 'T_cold'], X_train_cls.loc[battery_mask, 'init_Mg'],\n",
    "    c='red', edgecolors='k', s=50, label='Non-Battery Grade (red circles)'\n",
    ")\n",
    "plt.scatter(\n",
    "    X_train_cls.loc[non_battery_mask, 'T_cold'], X_train_cls.loc[non_battery_mask, 'init_Mg'],\n",
    "    c='blue', edgecolors='k', s=50, label='Battery Grade (blue circles)'\n",
    ")\n",
    "\n",
    "plt.xlabel('Cold Reactor Temperature (°C)')\n",
    "plt.ylabel('Initial Mg Concentration (ppm)')\n",
    "plt.title('GPC Decision Boundary for Battery-Grade Outcomes')\n",
    "plt.ylim(0, 12000)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Ray Tracing for Experiment Suggestion\n",
    "\n",
    "To further refine the decision boundary with maximum efficiency, we used a ray-tracing-inspired algorithm. This method identifies pairs of nearest-neighboring battery-grade and non-battery-grade experiments and calculates their mathematical midpoint. The midpoints lying closest to the GPC's 0.5 probability boundary are then suggested as the next set of experiments to run.\n",
    "\n",
    "The plot below visualizes this process. It shows the experimental data, the GPC decision boundary, and the proposed new experiments (green triangles) located strategically to provide the most information for refining the boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Boundary Exploitation Using GPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray Tracing for finding midpoint suggestions\n",
    "feature_columns_distance = ['T_cold', 'init_Mg']\n",
    "\n",
    "battery_grade_df = cleaned_data[cleaned_data['bg']==0][feature_columns_distance].copy()\n",
    "non_battery_grade_df = cleaned_data[cleaned_data['bg']==1][feature_columns_distance].copy()\n",
    "\n",
    "suggestion_df = pd.DataFrame()\n",
    "for i_bg, row_bg in battery_grade_df.iterrows():\n",
    "    for i_non_bg, row_non_bg in non_battery_grade_df.iterrows():\n",
    "        new_row = (row_bg + row_non_bg) / 2\n",
    "        suggestion_df = pd.concat([suggestion_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Filter suggestions to those near the boundary\n",
    "suggestion_df_scaled = scaler_cls.transform(suggestion_df)\n",
    "suggestion_probs = gpc_model.predict_proba(suggestion_df_scaled)[:, 1]\n",
    "near_boundary_data = suggestion_df[np.abs(suggestion_probs - 0.5) < 0.1]\n",
    "\n",
    "# Re-plot with suggestions\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contour(T_cold_mesh, init_Mg_mesh, prob_mesh, levels=[0.5], colors='red', linestyles='--', linewidths=2, label='Decision Boundary')\n",
    "plt.scatter(X_train_cls['T_cold'], X_train_cls['init_Mg'], c=y_train_cls, cmap='coolwarm', edgecolors='k', s=50, label='Experimental Data')\n",
    "plt.scatter(near_boundary_data['T_cold'], near_boundary_data['init_Mg'], c='green', marker='^', s=80, edgecolors='k', label='Ray Tracing Suggestions')\n",
    "\n",
    "plt.xlabel('Cold Reactor Temperature (°C)')\n",
    "plt.ylabel('Initial Mg Concentration (ppm)')\n",
    "plt.title('Ray Tracing Experiment Suggestions Near Decision Boundary')\n",
    "plt.ylim(0, 12000)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-by-Step Ray Tracing and Decision Boundary Extraction Using GPR (to show case) and GPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "# Step 1: Create binary classification label for battery-grade\n",
    "train_set = cleaned_data.copy()\n",
    "train_set['bg'] = np.where(train_set['fini_Mg'] < 80, 0, 1)\n",
    "\n",
    "# Step 2: Select features and target\n",
    "X_train_raytracing = train_set[['T_cold', 'init_Mg']]\n",
    "y_train_raytracing = train_set['bg']\n",
    "\n",
    "# Step 3: Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raytracing)\n",
    "\n",
    "# Step 4: Create 2D meshgrid for T_cold and init_Mg\n",
    "T_cold_range = np.linspace(X_train_raytracing['T_cold'].min(), X_train_raytracing['T_cold'].max(), 500)\n",
    "init_Mg_range = np.linspace(-500, 12000, 500)\n",
    "T_cold_mesh, init_Mg_mesh = np.meshgrid(T_cold_range, init_Mg_range)\n",
    "mesh_points = np.vstack([T_cold_mesh.ravel(), init_Mg_mesh.ravel()]).T\n",
    "mesh_points_scaled = scaler.transform(mesh_points)\n",
    "\n",
    "# Step 5: Train GPR\n",
    "gpr = GaussianProcessRegressor(kernel=RBF(length_scale=[0.3, 0.3]), alpha=0.06)\n",
    "gpr.fit(X_train_scaled, y_train_raytracing)\n",
    "gpr_probs = gpr.predict(mesh_points_scaled).reshape(T_cold_mesh.shape)\n",
    "\n",
    "# Step 6: Train GPC\n",
    "gpc_kernel = ConstantKernel(1.0) * (Matern(length_scale=[0.3, 0.3], nu=1.5) + WhiteKernel(noise_level=0.06))\n",
    "gpc = GaussianProcessClassifier(kernel=gpc_kernel, n_restarts_optimizer=10, max_iter_predict=100)\n",
    "gpc.fit(X_train_scaled, y_train_raytracing)\n",
    "gpc_probs = gpc.predict_proba(mesh_points_scaled)[:, 1].reshape(T_cold_mesh.shape)\n",
    "\n",
    "# Step 7: Transform and score suggestions\n",
    "X_sugg_scaled = scaler.transform(suggestion_df[['T_cold', 'init_Mg']])\n",
    "gpr_sugg_probs = gpr.predict(X_sugg_scaled)\n",
    "gpc_sugg_probs = gpc.predict_proba(X_sugg_scaled)[:, 1]\n",
    "near_gpr = suggestion_df[np.abs(gpr_sugg_probs - 0.5) < 0.05]\n",
    "near_gpc = suggestion_df[np.abs(gpc_sugg_probs - 0.5) < 0.05]\n",
    "\n",
    "# Custom legend handles\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', label='Battery Grade (0)', markerfacecolor='purple', markeredgecolor='k', markersize=7),\n",
    "    Line2D([0], [0], marker='o', color='w', label='Non-Battery Grade (1)', markerfacecolor='yellowgreen', markeredgecolor='k', markersize=7),\n",
    "    Line2D([0], [0], marker='x', color='green', label='Suggestions', linestyle='None', markersize=8),\n",
    "    Line2D([0], [0], marker='^', color='orange', label='Near Boundary Suggestions', markeredgecolor='k', linestyle='None', markersize=10),\n",
    "    Line2D([0], [0], color='red', lw=2, linestyle='--', label='GPR Boundary'),\n",
    "    Line2D([0], [0], color='blue', lw=2, linestyle='--', label='GPC Boundary')\n",
    "]\n",
    "\n",
    "# Plot 1: GPR Decision Boundary\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.contour(T_cold_mesh, init_Mg_mesh, gpr_probs, levels=[0.5], colors='red', linestyles='--', linewidths=2)\n",
    "scatter = plt.scatter(X_train_raytracing['T_cold'], X_train_raytracing['init_Mg'], c=y_train_raytracing, cmap='viridis', edgecolors='k', s=30)\n",
    "plt.colorbar(scatter, ticks=[0, 1]).set_label('Battery Grade')\n",
    "plt.title('GPR Decision Boundary')\n",
    "plt.xlabel('T_cold'); plt.ylabel('init_Mg')\n",
    "plt.ylim(-500, 12000)\n",
    "plt.legend(handles=legend_elements[:2] + [legend_elements[4]])\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Plot 2: GPR + Suggestions\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.contour(T_cold_mesh, init_Mg_mesh, gpr_probs, levels=[0.5], colors='red', linestyles='--', linewidths=2)\n",
    "plt.scatter(X_train_raytracing['T_cold'], X_train_raytracing['init_Mg'], c=y_train_raytracing, cmap='viridis', edgecolors='k', s=30)\n",
    "plt.scatter(suggestion_df['T_cold'], suggestion_df['init_Mg'], c='green', marker='x', s=30)\n",
    "plt.colorbar(scatter, ticks=[0, 1]).set_label('Battery Grade')\n",
    "plt.title('GPR + Suggested Experiments')\n",
    "plt.xlabel('T_cold'); plt.ylabel('init_Mg')\n",
    "plt.legend(handles=legend_elements[:3] + [legend_elements[4]])\n",
    "plt.ylim(-500, 12000); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Plot 3: GPR Near-Boundary\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.contour(T_cold_mesh, init_Mg_mesh, gpr_probs, levels=[0.5], colors='red', linestyles='--', linewidths=2)\n",
    "plt.scatter(X_train_raytracing['T_cold'], X_train_raytracing['init_Mg'], c=y_train_raytracing, cmap='viridis', edgecolors='k', s=30)\n",
    "plt.scatter(near_gpr['T_cold'], near_gpr['init_Mg'], color='orange', marker='^', s=70, edgecolors='k')\n",
    "plt.colorbar(scatter, ticks=[0, 1]).set_label('Battery Grade')\n",
    "plt.title('GPR Near-Decision Suggestions')\n",
    "plt.xlabel('T_cold'); plt.ylabel('init_Mg')\n",
    "plt.legend(handles=legend_elements[:-1])\n",
    "plt.ylim(-500, 12000); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Plot 4: GPC Decision Boundary\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.contour(T_cold_mesh, init_Mg_mesh, gpc_probs, levels=[0.5], colors='blue', linestyles='--', linewidths=2)\n",
    "scatter = plt.scatter(X_train_raytracing['T_cold'], X_train_raytracing['init_Mg'], c=y_train_raytracing, cmap='viridis', edgecolors='k', s=30)\n",
    "plt.colorbar(scatter, ticks=[0, 1]).set_label('Battery Grade')\n",
    "plt.title('GPC Decision Boundary')\n",
    "plt.xlabel('T_cold'); plt.ylabel('init_Mg')\n",
    "plt.legend(handles=legend_elements[:2] + [legend_elements[5]])\n",
    "plt.ylim(-500, 12000); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Plot 5: GPC + Suggestions\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.contour(T_cold_mesh, init_Mg_mesh, gpc_probs, levels=[0.5], colors='blue', linestyles='--', linewidths=2)\n",
    "plt.scatter(X_train_raytracing['T_cold'], X_train_raytracing['init_Mg'], c=y_train_raytracing, cmap='viridis', edgecolors='k', s=30)\n",
    "plt.scatter(suggestion_df['T_cold'], suggestion_df['init_Mg'], c='green', marker='x', s=30)\n",
    "plt.colorbar(scatter, ticks=[0, 1]).set_label('Battery Grade')\n",
    "plt.title('GPC + Suggested Experiments')\n",
    "plt.xlabel('T_cold'); plt.ylabel('init_Mg')\n",
    "plt.legend(handles=legend_elements[:3] + [legend_elements[5]])\n",
    "plt.ylim(-500, 12000); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Plot 6: GPC Near-Boundary\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.contour(T_cold_mesh, init_Mg_mesh, gpc_probs, levels=[0.5], colors='blue', linestyles='--', linewidths=2)\n",
    "plt.scatter(X_train_raytracing['T_cold'], X_train_raytracing['init_Mg'], c=y_train_raytracing, cmap='viridis', edgecolors='k', s=30)\n",
    "plt.scatter(near_gpc['T_cold'], near_gpc['init_Mg'], color='orange', marker='^', s=70, edgecolors='k')\n",
    "plt.colorbar(scatter, ticks=[0, 1]).set_label('Battery Grade')\n",
    "plt.title('GPC Near-Decision Suggestions')\n",
    "plt.xlabel('T_cold'); plt.ylabel('init_Mg')\n",
    "plt.legend(handles=legend_elements[:-2] + [legend_elements[5]])\n",
    "plt.ylim(-500, 12000); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Results and Analysis\n",
    "\n",
    "This section presents the final analyses performed on the complete dataset of 80 experiments, corresponding to Figure 8 in the paper. We examine the distributions of key parameters for battery-grade versus non-battery-grade outcomes and conduct a final feature importance analysis to confirm our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cleaned_data_all' in locals() and not cleaned_data_all.empty:\n",
    "    find_missing_data(cleaned_data_all)\n",
    "\n",
    "    feature_columns = ['T_cold', 'T_hot', 'delta_T', 'flow_rate', 'slurry_concentration', 'init_Ca', 'init_K', 'init_Li', 'init_Mg', 'init_Na']\n",
    "    target_columns_mg = ['fini_Mg']\n",
    "    analysis_df_mg = cleaned_data_all.copy()\n",
    "    analysis_df_mg['random'] = np.random.random(size=len(analysis_df_mg))\n",
    "    feature_columns_with_random = feature_columns + ['random']\n",
    "    \n",
    "    # Perform analysis for Final Mg Concentration\n",
    "    perform_full_analysis(analysis_df_mg.dropna(subset=feature_columns_with_random + target_columns_mg), \n",
    "                          feature_columns_with_random, \n",
    "                          target_columns_mg)\n",
    "\n",
    "    # Perform analysis for Battery Grade Label\n",
    "    target_columns_bg = ['bg']\n",
    "    analysis_df_bg = cleaned_data_all.copy().dropna(subset=feature_columns + target_columns_bg)\n",
    "    analysis_df_bg['random'] = np.random.random(size=len(analysis_df_bg))\n",
    "    perform_full_analysis(analysis_df_bg, feature_columns_with_random, target_columns_bg)\n",
    "else:\n",
    "    print(\"Skipping analysis because data was not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparative Simulation\n",
    "\n",
    "To quantify the benefit of the HITL-AL approach, we conducted a computational experiment comparing its efficacy against two non-human-in-the-loop active learning frameworks, as shown in Figure 9 of the paper. We created a large surrogate dataset of 10,000,000 simulated experiments, labeled using a GPC model trained on our final data. We then created two versions of this dataset:\n",
    "\n",
    "-   **Uninformed Dataset**: Adheres to the initial, broad parameter ranges.\n",
    "-   **Informed Dataset**: Constrained by the optimal temperature ranges discovered during the HITL-AL process.\n",
    "\n",
    "We then simulated three data acquisition strategies over 100 trials each:\n",
    "1.  **HITL-AL**: Our result, finding battery-grade conditions in 38 experiments.\n",
    "2.  **Simplified Bayesian Optimization**: An automated approach using Upper Confidence Bound (UCB) to select experiments.\n",
    "3.  **Random Sampling**: A baseline approach selecting experiments randomly.\n",
    "\n",
    "The plot below shows the success rate of each method in identifying a battery-grade condition within 40 experiments. The results clearly show that incorporating human-derived knowledge (`Informed` datasets) dramatically improves the performance of automated methods, and that the HITL-AL approach was highly effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. Detailed Comparative Analysis: Loading and Plotting Results\n",
    "\n",
    "In this section, we demonstrate how to load the detailed simulation results and create comprehensive polynomial fit analyses that show the convergence behavior of different active learning strategies over time. This analysis provides deeper insights into the performance characteristics of each method beyond simple success rates.\n",
    "\n",
    "#### 8.1.1. Loading Simulation Data and Creating Polynomial Fit Plots\n",
    "\n",
    "The following cell loads the detailed simulation results from the computational experiments and creates polynomial fit plots with confidence intervals. This analysis shows how each method's success rate evolves as the number of experiments increases, providing insights into the learning efficiency and convergence behavior of different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict_poly(x, y, degree, num_points=100):\n",
    "    \"\"\"\n",
    "    Fit a polynomial to the data and predict with confidence intervals.\n",
    "    \n",
    "    This function performs polynomial regression and calculates prediction uncertainty\n",
    "    using the residual variance and design matrix approach.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array-like\n",
    "        Input features (number of experiments)\n",
    "    y : array-like\n",
    "        Target values (success rates)\n",
    "    degree : int\n",
    "        Degree of polynomial to fit\n",
    "    num_points : int\n",
    "        Number of points for prediction\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    x_pred : array\n",
    "        Prediction x values\n",
    "    y_pred : array\n",
    "        Predicted y values\n",
    "    std_pred : array\n",
    "        Standard deviation of predictions (confidence intervals)\n",
    "    \"\"\"\n",
    "    coeffs, residuals, _, _, _ = np.polyfit(x, y, degree, full=True)\n",
    "    poly = np.poly1d(coeffs)\n",
    "    x_pred = np.linspace(min(x), max(x), num_points)\n",
    "    y_pred = poly(x_pred)\n",
    "    residual_variance = residuals / (len(y) - (degree + 1))\n",
    "    X_design = np.vander(x_pred, N=degree + 1)\n",
    "    prediction_variance = np.dot(X_design, np.dot(np.linalg.pinv(np.dot(X_design.T, X_design)), X_design.T)) * residual_variance\n",
    "    std_pred = np.sqrt(np.diag(prediction_variance))\n",
    "    return x_pred, y_pred, std_pred\n",
    "\n",
    "# Load detailed simulation results\n",
    "print(\"Loading detailed simulation results...\")\n",
    "\n",
    "# Check if .npy files exist and load them\n",
    "try:\n",
    "    bayesian_outcome_inf = np.load('Data\\\\generated\\\\comparative_analysis\\\\bayesian_outcome_inf.npy')\n",
    "    bayesian_outcome_uninf = np.load('Data\\\\generated\\\\comparative_analysis\\\\bayesian_outcome_uninf.npy')\n",
    "    random_aq_ap2_inf = np.load('Data\\\\generated\\\\comparative_analysis\\\\random_aq_ap2_inf.npy')\n",
    "    random_aq_ap2_uninf = np.load('Data\\\\generated\\\\comparative_analysis\\\\random_aq_ap2_uninf.npy')\n",
    "    \n",
    "    print(\"✓ Successfully loaded all simulation data files\")\n",
    "    print(f\"  - Bayesian Informed: {len(bayesian_outcome_inf)} experiments\")\n",
    "    print(f\"  - Bayesian Uninformed: {len(bayesian_outcome_uninf)} experiments\")\n",
    "    print(f\"  - Random Informed: {len(random_aq_ap2_inf)} experiments\")\n",
    "    print(f\"  - Random Uninformed: {len(random_aq_ap2_uninf)} experiments\")\n",
    "    \n",
    "    # Create x_values (number of experiments)\n",
    "    x_values = np.arange(100)\n",
    "    \n",
    "    # Create comprehensive polynomial fit plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Define colors, labels, and markers for each dataset\n",
    "    colors = ['red', 'green', 'blue', 'purple']\n",
    "    labels = [\"Bayesian - Informed\", \"Bayesian - Uninformed\", \"Random - Informed\", \"Random - Uninformed\"]\n",
    "    datasets = [bayesian_outcome_inf, bayesian_outcome_uninf, random_aq_ap2_inf, random_aq_ap2_uninf]\n",
    "    markers = ['o', 's', '^', 'x']  # Different markers for each dataset\n",
    "    \n",
    "    # Fit polynomial and plot for each dataset\n",
    "    degree = 4  # 4th degree polynomial for smooth fitting\n",
    "    for data, color, label, marker in zip(datasets, colors, labels, markers):\n",
    "        # Fit polynomial and get predictions with confidence intervals\n",
    "        x_pred, y_pred, std = fit_and_predict_poly(x_values, data, degree)\n",
    "        \n",
    "        # Plot original data points\n",
    "        ax.plot(x_values, data, marker, color=color, markersize=4, label=f'{label}')\n",
    "        \n",
    "        # Plot polynomial fit line\n",
    "        ax.plot(x_pred, y_pred, '-', color=color, linewidth=2, label=f'Polynomial Fit - {label}')\n",
    "        \n",
    "        # Plot confidence intervals (95% confidence)\n",
    "        ax.fill_between(x_pred, y_pred - 1.96 * std, y_pred + 1.96 * std, \n",
    "                       color=color, alpha=0.2, label=f'95% CI - {label}')\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_xlabel('Number of Experiments', fontsize=12)\n",
    "    ax.set_ylabel('Success Rate (Count)', fontsize=12)\n",
    "    ax.set_title('Comparative Analysis: Active Learning Strategy Performance\\nPolynomial Fits with 95% Confidence Intervals', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0., fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_ylim(0, max([max(d) for d in datasets]) * 1.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed analysis at specific point\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETAILED ANALYSIS AT x = 38 EXPERIMENTS\")\n",
    "    print(\"=\"*60)\n",
    "    x_point = 38\n",
    "    for data, label in zip(datasets, labels):\n",
    "        coeffs, residuals, _, _, _ = np.polyfit(x_values, data, degree, full=True)\n",
    "        poly = np.poly1d(coeffs)\n",
    "        X_design_point = np.vander([x_point], N=degree + 1)\n",
    "        residual_variance = residuals / (len(data) - (degree + 1))\n",
    "        prediction_variance = np.dot(X_design_point, np.dot(np.linalg.pinv(np.dot(X_design_point.T, X_design_point)), X_design_point.T)) * residual_variance\n",
    "        std_point = np.sqrt(prediction_variance)\n",
    "        predicted_value = poly(x_point)\n",
    "        print(f\"{label:25s} - Success Rate: {predicted_value:6.2f} ± {std_point[0][0]:.2f} (95% CI)\")\n",
    "    \n",
    "    # Create the second plot with highlighted analysis point\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    for data, color, label, marker in zip(datasets, colors, labels, markers):\n",
    "        x_pred, y_pred, std = fit_and_predict_poly(x_values, data, degree)\n",
    "        ax.plot(x_values, data, marker, color=color, markersize=4, label=f'{label}')\n",
    "        ax.plot(x_pred, y_pred, '-', color=color, linewidth=2, label=f'Polynomial Fit - {label}')\n",
    "        ax.fill_between(x_pred, y_pred - 1.96 * std, y_pred + 1.96 * std, color=color, alpha=0.2)\n",
    "    \n",
    "    # Highlight specific analysis point (x=40 as mentioned in the paper)\n",
    "    ax.axvline(x=40, color='black', linestyle='--', linewidth=3, label='Analysis Point (x=40) - HITL Success Point')\n",
    "    \n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_ylim(-1, 100)\n",
    "    ax.set_xlabel('Number of Experiments', fontsize=12)\n",
    "    ax.set_ylabel('Success Rate (Count)', fontsize=12)\n",
    "    ax.set_title('Comparative Analysis with Highlighted Performance Threshold\\n(40 Experiments = HITL-AL Achievement)', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0., fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n✓ Polynomial fit analysis completed successfully!\")\n",
    "    print(\"  - Red: Bayesian Informed (best performance)\")\n",
    "    print(\"  - Green: Bayesian Uninformed (good performance)\")\n",
    "    print(\"  - Blue: Random Informed (moderate performance)\")\n",
    "    print(\"  - Purple: Random Uninformed (baseline performance)\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"⚠️  Warning: {e}\")\n",
    "    print(\"   The detailed simulation data files (.npy) are not available.\")\n",
    "    print(\"   This analysis requires the results from the computational experiments.\")\n",
    "    print(\"   Please ensure the following files are in the current directory:\")\n",
    "    print(\"   - bayesian_outcome_inf.npy\")\n",
    "    print(\"   - bayesian_outcome_uninf.npy\")\n",
    "    print(\"   - random_aq_ap2_inf.npy\")\n",
    "    print(\"   - random_aq_ap2_uninf.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Comparative Analysis: HITL vs Bayesian vs Random Sampling in Informed and Uninformed Scenarios\n",
    "\n",
    "This section demonstrates the complete pipeline for comparing Bayesian active learning with random sampling in both informed and uninformed experimental setups. We'll show how the system learns to find successful crystallization conditions through iterative sampling.\n",
    "\n",
    "#### 8.2.1 Informed Scenario: Full Parameter Range Knowledge\n",
    "\n",
    "First, let's set up the informed scenario where we have complete knowledge of the parameter ranges and boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter bounds for informed scenario\n",
    "bounds_ap2_inf = {\n",
    "    \"T_cold\": (5, 80),\n",
    "    \"T_hot\": (6, 90),  # Increased upper limit by 20C for later adjustment\n",
    "    \"flow_rate\": (0.5, 6),\n",
    "    \"slurry_concentration\": (1.5, 10),\n",
    "    \"init_Ca\": (2000, 300000),  # Adjusted for +/- 30%\n",
    "    \"init_K\": (100, 5000),      # Adjusted for +/- 30%\n",
    "    \"init_Li\": (2000, 200000), # Adjusted for +/- 30%\n",
    "    \"init_Mg\": (10, 100000),      # Adjusted for +/- 30%\n",
    "    \"init_Na\": (100, 20000)     # Adjusted for +/- 30%\n",
    "}\n",
    "\n",
    "# Generate surrogate data using Latin Hypercube Sampling\n",
    "surrogate_data = sift_lhs_sample(10_000_000, bounds=bounds_ap2_inf, lhs_sampler=False, delta_T=0, seed=0)\n",
    "\n",
    "# Ensure T_cold is not higher than T_hot by swapping if needed\n",
    "mask = surrogate_data['T_cold'] > surrogate_data['T_hot']\n",
    "surrogate_data.loc[mask, ['T_cold', 'T_hot']] = surrogate_data.loc[mask, ['T_hot', 'T_cold']].values\n",
    "surrogate_data['T_hot'] = surrogate_data['T_hot'] + 5  # Increase T_hot by 5C\n",
    "surrogate_data['delta_T'] = surrogate_data['T_hot'] - surrogate_data['T_cold']\n",
    "\n",
    "print(f\"Generated {len(surrogate_data):,} surrogate data points for informed scenario\")\n",
    "print(f\"Parameter ranges: {surrogate_data.describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.2 Surrogate Model Training with Gaussian Process Classification\n",
    "\n",
    "Now we'll train a Gaussian Process Classification model on our experimental data to predict success probability in the surrogate space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data from experimental results\n",
    "train_set = cleaned_data[['experiment_id', 'T_cold', 'T_hot', 'flow_rate',\n",
    "       'slurry_concentration',  'init_Ca', 'init_K', 'init_Li', 'init_Mg',\n",
    "       'init_Na',  'fini_Ca',\n",
    "       'fini_K', 'fini_Li', 'fini_Mg', 'fini_Na',\n",
    "       'fini_Li_purity','delta_T',\n",
    "       'delta_Mg', 'bg']].copy()\n",
    "train_set.dropna(inplace=True)\n",
    "train_set['bg'] = np.where(train_set['fini_Mg']<100, 0, 1)\n",
    "\n",
    "# Prepare features for GPC training\n",
    "bg_X_train = train_set[['T_cold', 'T_hot', 'delta_T', 'flow_rate',\n",
    "                            'slurry_concentration','init_Ca', 'init_K',\n",
    "                            'init_Li', 'init_Mg', 'init_Na']].copy()\n",
    "\n",
    "# Scale the training features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(bg_X_train)\n",
    "bg_X_train_scaled = scaler.transform(bg_X_train)\n",
    "\n",
    "bg_y_train = train_set['bg'].copy()\n",
    "\n",
    "# Train Gaussian Process Classifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "\n",
    "gpc_model_surrogate = GaussianProcessClassifier(\n",
    "    kernel=Matern(length_scale=[0.3]*10, nu=1.5), \n",
    "    random_state=42\n",
    ")\n",
    "gpc_model_surrogate.fit(bg_X_train_scaled, bg_y_train)\n",
    "\n",
    "print(f\"GPC Model trained on {len(bg_X_train)} experimental samples\")\n",
    "print(f\"Kernel: {gpc_model_surrogate.kernel_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.3 Surrogate Space Prediction and Labeling\n",
    "\n",
    "Now we'll use our trained GPC model to predict success probabilities across the entire surrogate space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare surrogate test features\n",
    "bg_X_test = surrogate_data[['T_cold', 'T_hot', 'delta_T', 'flow_rate',\n",
    "                            'slurry_concentration','init_Ca', 'init_K',\n",
    "                            'init_Li', 'init_Mg', 'init_Na']].copy()\n",
    "\n",
    "# Scale surrogate data using the same scaler\n",
    "X_test_scaled = scaler.transform(bg_X_test)\n",
    "\n",
    "# Predict success probabilities for all surrogate points\n",
    "y_pred_proba = gpc_model_surrogate.predict_proba(X_test_scaled)\n",
    "surrogate_data['bg_prob'] = y_pred_proba[:, 1]  # Probability of success (bg=0)\n",
    "surrogate_data['bg'] = np.where(surrogate_data['bg_prob'] < 0.5, 0, 1)\n",
    "\n",
    "# Save the labeled surrogate data\n",
    "#np.save('surrogate_data_10_000_000_2024_07_17.npy', surrogate_data.to_numpy())\n",
    "\n",
    "#print(f\"Surrogate space labeled and saved\")\n",
    "#print(f\"Success rate in surrogate space: {(surrogate_data['bg'] == 0).mean():.3f}\")\n",
    "#print(f\"Total successful conditions: {(surrogate_data['bg'] == 0).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sections 8.2.4 - 8.2.8: Comparative Analysis Implementation**\n",
    "\n",
    "**Note:** The following cells reproduce the computational analysis demonstrated in **Section 8.2** (Comparative Analysis) and **Section 9** (Results and Discussion) of the main paper.\n",
    "\n",
    "##### **What These Sections Do:**\n",
    "\n",
    "- **Section 8.2.4**: Bayesian Active Learning Analysis (Informed Scenario) using GPR-based approach\n",
    "- **Section 8.2.5**: Generation of uninformed surrogate data space for comparison\n",
    "- **Section 8.2.6**: Bayesian Active Learning Analysis (Uninformed Scenario) using GPR-based approach  \n",
    "- **Section 8.2.7**: Random Sampling Analysis (Uninformed Scenario) for baseline comparison\n",
    "- **Section 8.2.8**: Comprehensive comparative analysis and statistical evaluation\n",
    "\n",
    "##### **⚠️ Important Execution Note:**\n",
    "\n",
    "**These computations are computationally intensive and time-consuming** (can take several hours to complete). To preserve notebook execution time and resources, the computational cells are **commented out by default**.\n",
    "\n",
    "**To run the full analysis:**\n",
    "1. **Uncomment the computational cells** in sections 8.2.4-8.2.7\n",
    "2. **Ensure you have sufficient computational resources** (recommended: 16GB+ RAM, multi-core CPU)\n",
    "3. **Be prepared for extended execution times** (2-6 hours depending on your system)\n",
    "\n",
    "**For demonstration purposes**, the results are loaded from pre-computed files in the `Data/generated/comparative_analysis/` directory, allowing you to see the final plots and analysis without running the full computations.\n",
    "\n",
    "**Uncomment only when you need to reproduce the complete analysis or modify the experimental parameters.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.4 Bayesian Active Learning Analysis\n",
    "\n",
    "Now we'll conduct Bayesian active learning to see how many steps it takes to find a successful condition, repeating this multiple times for statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize results array for informed Bayesian learning\n",
    "# bayesian_outcome_inf = np.zeros(100)\n",
    "# test_size = 10000\n",
    "\n",
    "# print(\"Conducting Bayesian Active Learning Analysis (Informed)...\")\n",
    "# for cycles_num in tqdm(range(len(bayesian_outcome_inf))):\n",
    "    \n",
    "#     repetition = 100\n",
    "#     for r in range(repetition):\n",
    "#         # Randomly sample initial training set and test set\n",
    "#         train_set_init = surrogate_data.sample(2, random_state=np.random.randint(low=0, high=10000))\n",
    "#         test_data = surrogate_data.sample(test_size, random_state=np.random.randint(low=0, high=10000))\n",
    "        \n",
    "#         kappa = 2  # Exploration-exploitation parameter\n",
    "        \n",
    "#         # Active learning cycles\n",
    "#         for cycle in range(cycles_num):\n",
    "            \n",
    "#             # Prepare training data\n",
    "#             X_train = train_set_init[['T_cold', 'T_hot', 'delta_T', 'flow_rate',\n",
    "#                                'slurry_concentration', 'init_Ca', 'init_K',\n",
    "#                                'init_Li', 'init_Mg', 'init_Na']].copy()\n",
    "#             y_train = train_set_init['bg'].copy()\n",
    "            \n",
    "#             # Scale features\n",
    "#             scaler = StandardScaler()\n",
    "#             X_train_scaled = scaler.fit_transform(X_train)\n",
    "            \n",
    "#             # Train GPR model (NOT GPC - this avoids the 2-class requirement)\n",
    "#             gpr_model = GaussianProcessRegressor(\n",
    "#                 kernel=Matern(length_scale=[0.3]*10, nu=1.5), \n",
    "#                 alpha=0.25,\n",
    "#                 random_state=42\n",
    "#             )\n",
    "#             gpr_model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "#             # Predict on test data\n",
    "#             test_data_scaled = scaler.transform(test_data[['T_cold', 'T_hot', 'delta_T', 'flow_rate',\n",
    "#                                  'slurry_concentration', 'init_Ca', 'init_K',\n",
    "#                                  'init_Li', 'init_Mg', 'init_Na']].copy())\n",
    "            \n",
    "#             # Get predictions and uncertainties from GPR\n",
    "#             y_pred, sigma = gpr_model.predict(test_data_scaled, return_std=True)\n",
    "            \n",
    "#             # Convert GPR predictions to probabilities (0-1 range)\n",
    "#             # Clamp predictions to [0,1] range for probability interpretation\n",
    "#             y_pred_prob = np.clip(y_pred, 0, 1)\n",
    "            \n",
    "#             # Calculate acquisition function (UCB for regression-based classification)\n",
    "#             # We want to minimize the predicted value (closer to 0 = success)\n",
    "#             # Lower prediction + higher uncertainty = more exploration\n",
    "#             UCB = y_pred_prob - kappa * sigma\n",
    "            \n",
    "#             # Find the most promising sample (lowest UCB = best chance of success)\n",
    "#             index_min_ucb = np.argmin(UCB)\n",
    "#             best_sample = test_data.iloc[index_min_ucb]\n",
    "            \n",
    "#             # Check if we found a successful condition\n",
    "#             if best_sample['bg'] == 0:  # Success condition\n",
    "#                 bayesian_outcome_inf[cycles_num] += 1\n",
    "#                 break\n",
    "            \n",
    "#             # Add this point to the training set\n",
    "#             train_set_init = pd.concat([train_set_init, best_sample.to_frame().T], ignore_index=True)\n",
    "            \n",
    "#             # Reduce exploration over time\n",
    "#             kappa *= 0.95\n",
    "\n",
    "# # # Save results\n",
    "# # np.save('bayesian_outcome_inf.npy', bayesian_outcome_inf)\n",
    "# # print(f\"Informed Bayesian active learning analysis completed\")\n",
    "# # print(f\"Success rates: {bayesian_outcome_inf[:10]}\")  # Show first 10 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.5 Uninformed Scenario Analysis\n",
    "\n",
    "Now let's repeat the analysis for the uninformed scenario where we have limited knowledge of parameter ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define broader, less informed parameter bounds\n",
    "# bounds_ap2_uninf = {\n",
    "#     \"T_cold\": (0, 100),      # Broader range\n",
    "#     \"T_hot\": (0, 120),       # Broader range\n",
    "#     \"flow_rate\": (0, 10),    # Broader range\n",
    "#     \"slurry_concentration\": (0, 15),  # Broader range\n",
    "#     \"init_Ca\": (0, 500000),  # Broader range\n",
    "#     \"init_K\": (0, 10000),    # Broader range\n",
    "#     \"init_Li\": (0, 300000),  # Broader range\n",
    "#     \"init_Mg\": (0, 200000),  # Broader range\n",
    "#     \"init_Na\": (0, 50000)    # Broader range\n",
    "# }\n",
    "\n",
    "# # Generate uninformed surrogate data\n",
    "# surrogate_data_uninf = sift_lhs_sample(10_000_000, bounds=bounds_ap2_uninf, lhs_sampler=False, delta_T=0, seed=42)\n",
    "\n",
    "# # Apply basic constraints\n",
    "# mask = surrogate_data_uninf['T_cold'] > surrogate_data_uninf['T_hot']\n",
    "# surrogate_data_uninf.loc[mask, ['T_cold', 'T_hot']] = surrogate_data_uninf.loc[mask, ['T_hot', 'T_cold']].values\n",
    "# surrogate_data_uninf['T_hot'] = surrogate_data_uninf['T_hot'] + 5\n",
    "# surrogate_data_uninf['delta_T'] = surrogate_data_uninf['T_hot'] - surrogate_data_uninf['T_cold']\n",
    "\n",
    "# # Label uninformed surrogate data\n",
    "# bg_X_test_uninf = surrogate_data_uninf[['T_cold', 'T_hot', 'delta_T', 'flow_rate',\n",
    "#                             'slurry_concentration','init_Ca', 'init_K',\n",
    "#                             'init_Li', 'init_Mg', 'init_Na']].copy()\n",
    "\n",
    "# X_test_scaled_uninf = scaler.transform(bg_X_test_uninf)\n",
    "# y_pred_proba_uninf = gpc_model_surrogate.predict_proba(X_test_scaled_uninf)\n",
    "# surrogate_data_uninf['bg_prob'] = y_pred_proba_uninf[:, 1]\n",
    "# surrogate_data_uninf['bg'] = np.where(surrogate_data_uninf['bg_prob'] < 0.5, 0, 1)\n",
    "\n",
    "# print(f\"Uninformed surrogate space created with {len(surrogate_data_uninf):,} points\")\n",
    "# print(f\"Success rate in uninformed space: {(surrogate_data_uninf['bg'] == 0).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.6 Bayesian Active Learning for Uninformed Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize results array for uninformed Bayesian learning\n",
    "# bayesian_outcome_uninf = np.zeros(100)\n",
    "# test_size = 10000\n",
    "\n",
    "# print(\"Conducting Bayesian Active Learning Analysis (Uninformed)...\")\n",
    "# for cycles_num in tqdm(range(len(bayesian_outcome_uninf))):\n",
    "    \n",
    "#     repetition = 100\n",
    "#     for r in range(repetition):\n",
    "#         # Randomly sample initial training set and test set\n",
    "#         train_set_init = surrogate_data_uninf.sample(2, random_state=np.random.randint(low=0, high=10000))\n",
    "#         test_data = surrogate_data_uninf.sample(test_size, random_state=np.random.randint(low=0, high=10000))\n",
    "        \n",
    "#         kappa = 2\n",
    "        \n",
    "#         # Active learning cycles\n",
    "#         for cycle in range(cycles_num):\n",
    "            \n",
    "#             # Prepare training data\n",
    "#             X_train = train_set_init[['T_cold', 'T_hot', 'delta_T', 'flow_rate',\n",
    "#                                'slurry_concentration', 'init_Ca', 'init_K',\n",
    "#                                'init_Li', 'init_Mg', 'init_Na']].copy()\n",
    "#             y_train = train_set_init['bg'].copy()\n",
    "            \n",
    "#             # Scale features\n",
    "#             scaler = StandardScaler()\n",
    "#             X_train_scaled = scaler.fit_transform(X_train)\n",
    "            \n",
    "#             # Train GPR model (NOT GPC - this avoids the 2-class requirement)\n",
    "#             gpr_model = GaussianProcessRegressor(\n",
    "#                 kernel=Matern(length_scale=[0.3]*10, nu=1.5), \n",
    "#                 alpha=0.25,\n",
    "#                 random_state=42\n",
    "#             )\n",
    "#             gpr_model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "#             # Predict on test data\n",
    "#             test_data_scaled = scaler.transform(test_data[['T_cold', 'T_hot', 'delta_T', 'flow_rate',\n",
    "#                                  'slurry_concentration', 'init_Ca', 'init_K',\n",
    "#                                  'init_Li', 'init_Mg', 'init_Na']].copy())\n",
    "            \n",
    "#             # Get predictions and uncertainties from GPR\n",
    "#             y_pred, sigma = gpr_model.predict(test_data_scaled, return_std=True)\n",
    "            \n",
    "#             # Convert GPR predictions to probabilities\n",
    "#             y_pred_prob = np.clip(y_pred, 0, 1)\n",
    "            \n",
    "#             # Calculate acquisition function (UCB for regression-based classification)\n",
    "#             # Lower prediction + higher uncertainty = more exploration\n",
    "#             UCB = y_pred_prob - kappa * sigma\n",
    "            \n",
    "#             # Find the most promising sample (lowest UCB = best chance of success)\n",
    "#             index_min_ucb = np.argmin(UCB)\n",
    "#             best_sample = test_data.iloc[index_min_ucb]\n",
    "            \n",
    "#             # Check if we found a successful condition\n",
    "#             if best_sample['bg'] == 0:\n",
    "#                 bayesian_outcome_uninf[cycles_num] += 1\n",
    "#                 break\n",
    "            \n",
    "#             # Add this point to the training set\n",
    "#             train_set_init = pd.concat([train_set_init, best_sample.to_frame().T], ignore_index=True)\n",
    "            \n",
    "#             kappa *= 0.95\n",
    "\n",
    "# # # Save results\n",
    "# # np.save('bayesian_outcome_uninf.npy', bayesian_outcome_uninf)\n",
    "# # print(f\"Uninformed Bayesian active learning analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.7 Random Sampling for Uninformed Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize results array for uninformed random sampling\n",
    "# random_aq_ap2_uninf = np.zeros(100)\n",
    "# test_size = 10000\n",
    "\n",
    "# print(\"Conducting Random Sampling Analysis (Uninformed)...\")\n",
    "# for cycles_num in tqdm(range(len(random_aq_ap2_uninf))):\n",
    "    \n",
    "#     repetition = 100\n",
    "#     for r in range(repetition):\n",
    "#         # Randomly sample initial training set and test set\n",
    "#         train_set_init = surrogate_data_uninf.sample(2, random_state=np.random.randint(low=0, high=10000))\n",
    "#         test_data = surrogate_data_uninf.sample(test_size, random_state=np.random.randint(low=0, high=10000))\n",
    "        \n",
    "#         # Random sampling cycles\n",
    "#         for cycle in range(cycles_num):\n",
    "            \n",
    "#             # Randomly select a sample from test data\n",
    "#             random_index = np.random.randint(0, len(test_data))\n",
    "#             random_sample = test_data.iloc[random_index]\n",
    "            \n",
    "#             # Check if we found a successful condition\n",
    "#             if random_sample['bg'] == 0:\n",
    "#                 random_aq_ap2_uninf[cycles_num] += 1\n",
    "#                 break\n",
    "            \n",
    "#             # Add this point to the training set\n",
    "#             train_set_init = pd.concat([train_set_init, random_sample.to_frame().T], ignore_index=True)\n",
    "\n",
    "# # Save results\n",
    "# np.save('random_aq_ap2_uninf.npy', random_aq_ap2_uninf)\n",
    "# print(f\"Uninformed random sampling analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.8 Comparative Analysis and Visualization\n",
    "\n",
    "Now let's create comprehensive visualizations comparing all four scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all results\n",
    "bayesian_outcome_inf = np.load('Data\\\\generated\\\\comparative_analysis\\\\bayesian_outcome_inf.npy')\n",
    "random_aq_ap2_inf = np.load('Data\\\\generated\\\\comparative_analysis\\\\random_aq_ap2_inf.npy')\n",
    "bayesian_outcome_uninf = np.load('Data\\\\generated\\\\comparative_analysis\\\\bayesian_outcome_uninf.npy')\n",
    "random_aq_ap2_uninf = np.load('Data\\\\generated\\\\comparative_analysis\\\\random_aq_ap2_uninf.npy')\n",
    "\n",
    "# Create x-axis values\n",
    "x_values = np.arange(len(bayesian_outcome_inf))\n",
    "\n",
    "# Create comprehensive comparison plot\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Informed vs Uninformed Bayesian\n",
    "ax1.scatter(x_values, bayesian_outcome_inf, alpha=0.6, label='Informed Bayesian', color='blue')\n",
    "ax1.scatter(x_values, bayesian_outcome_uninf, alpha=0.6, label='Uninformed Bayesian', color='red')\n",
    "ax1.set_xlabel('Number of Active Learning Cycles')\n",
    "ax1.set_ylabel('Success Rate (%)')\n",
    "ax1.set_title('Bayesian Active Learning: Informed vs Uninformed')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Informed vs Uninformed Random\n",
    "ax2.scatter(x_values, random_aq_ap2_inf, alpha=0.6, label='Informed Random', color='green')\n",
    "ax2.scatter(x_values, random_aq_ap2_uninf, alpha=0.6, label='Uninformed Random', color='orange')\n",
    "ax2.set_xlabel('Number of Random Sampling Cycles')\n",
    "ax2.set_ylabel('Success Rate (%)')\n",
    "ax2.set_title('Random Sampling: Informed vs Uninformed')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Informed Bayesian vs Random\n",
    "ax3.scatter(x_values, bayesian_outcome_inf, alpha=0.6, label='Bayesian Active Learning', color='blue')\n",
    "ax3.scatter(x_values, random_aq_ap2_inf, alpha=0.6, label='Random Sampling', color='green')\n",
    "ax3.set_xlabel('Number of Cycles')\n",
    "ax3.set_ylabel('Success Rate (%)')\n",
    "ax3.set_title('Informed Scenario: Bayesian vs Random')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Uninformed Bayesian vs Random\n",
    "ax4.scatter(x_values, bayesian_outcome_uninf, alpha=0.6, label='Bayesian Active Learning', color='red')\n",
    "ax4.scatter(x_values, random_aq_ap2_uninf, alpha=0.6, label='Random Sampling', color='orange')\n",
    "ax4.set_xlabel('Number of Cycles')\n",
    "ax4.set_ylabel('Success Rate (%)')\n",
    "ax4.set_title('Uninformed Scenario: Bayesian vs Random')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n=== COMPARATIVE ANALYSIS SUMMARY ===\")\n",
    "print(f\"Informed Bayesian - Avg success rate: {bayesian_outcome_inf.mean():.2f}%\")\n",
    "print(f\"Informed Random - Avg success rate: {random_aq_ap2_inf.mean():.2f}%\")\n",
    "print(f\"Uninformed Bayesian - Avg success rate: {bayesian_outcome_uninf.mean():.2f}%\")\n",
    "print(f\"Uninformed Random - Avg success rate: {random_aq_ap2_uninf.mean():.2f}%\")\n",
    "\n",
    "print(f\"\\nBayesian advantage (informed): {bayesian_outcome_inf.mean() - random_aq_ap2_inf.mean():.2f}%\")\n",
    "print(f\"Bayesian advantage (uninformed): {bayesian_outcome_uninf.mean() - random_aq_ap2_uninf.mean():.2f}%\")\n",
    "print(f\"Information advantage (Bayesian): {bayesian_outcome_inf.mean() - bayesian_outcome_uninf.mean():.2f}%\")\n",
    "print(f\"Information advantage (Random): {random_aq_ap2_inf.mean() - random_aq_ap2_uninf.mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "This notebook has provided the code and narrative to reproduce the key findings of our research. We have demonstrated how a Human-in-the-Loop Active Learning (HITL-AL) framework successfully optimized the complex process of continuous lithium carbonate crystallization.\n",
    "\n",
    "The key takeaways are:\n",
    "-   The synergy between human expertise and AI-driven exploration led to the discovery of a non-obvious process parameter—the elevated temperature of the cold reactor—that was critical for efficient magnesium removal.\n",
    "-   The HITL-AL framework significantly accelerated the optimization process, requiring only 38 experiments to identify key conditions for producing battery-grade material from highly impure feedstocks.\n",
    "-   This approach substantially increased the process's tolerance to magnesium impurities, potentially reducing the need for costly pre-refinement steps and making lower-grade lithium sources more economically viable.\n",
    "\n",
    "The methods and results presented here represent a significant step towards the sustainable and economic extraction of lithium from North America’s rich but challenging brine resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hitl-lithium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
